{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>/*@font-face {\n",
       "\tfont-family: \"Computer Modern\";\n",
       "\tsrc: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunss.otf');\n",
       "}\n",
       "@font-face {\n",
       "\tfont-family: \"Computer Modern\";\n",
       "\tfont-weight: bold;\n",
       "\tsrc: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunsx.otf');\n",
       "}\n",
       "@font-face {\n",
       "\tfont-family: \"Computer Modern\";\n",
       "\tfont-style: oblique;\n",
       "\tsrc: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunsi.otf');\n",
       "}\n",
       "@font-face {\n",
       "\tfont-family: \"Computer Modern\";\n",
       "\tfont-weight: bold;\n",
       "\tfont-style: oblique;\n",
       "\tsrc: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunso.otf');\n",
       "}*/\n",
       "\n",
       ".navbar-brand, .current_kernel_logo {display:none}\n",
       ".container {\n",
       "    width:80%;    \n",
       "}\n",
       "\n",
       "h1 {\n",
       "\tfont-family: Helvetica, serif;\n",
       "}\n",
       "h4{\n",
       "\tmargin-top:12px;\n",
       "\tmargin-bottom: 3px;\n",
       "   }\n",
       "div.text_cell_render{\n",
       "\tfont-family: Computer Modern, \"Helvetica Neue\", Arial, Helvetica, Geneva, sans-serif;\n",
       "\tline-height: 145%;\n",
       "\tfont-size: 100%;\n",
       "\twidth:100%;\n",
       "\tmargin-left:auto;\n",
       "\tmargin-right:auto;\n",
       "}\n",
       ".CodeMirror{\n",
       "\t\tfont-family: \"Source Code Pro\", source-code-pro,Consolas, monospace;\n",
       "}\n",
       ".text_cell_render h5 {\n",
       "\tfont-weight: 300;\n",
       "\tfont-size: 22pt;\n",
       "\t/*color: #4057A1;*/\n",
       "\tfont-style: italic;\n",
       "\tmargin-bottom: .5em;\n",
       "\tmargin-top: 0.5em;\n",
       "\tdisplay: block;\n",
       "}\n",
       "\n",
       ".warning{\n",
       "\tcolor: rgb( 240, 20, 20 )\n",
       "\t}   \n",
       "\n",
       "div.spoiler {\n",
       "\tdisplay: none;\n",
       "}\n",
       "\n",
       ".rendered_html code {\n",
       "\tborder: 0;\n",
       "\t/*background-color: #eee;*/\n",
       "\tfont-size: 100%;\n",
       "\tpadding: 1px 2px;\n",
       "}\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import css_from_file\n",
    "css_from_file('style/style.css')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modeling pipelines\n",
    "=================\n",
    "\n",
    "Thinking about modeling as a series of transformations is really helpful.\n",
    "Pipelines and functional transformations are the cleanest way to preprocess the data.\n",
    "It has its roots in Category theory from mathematics.\n",
    "\n",
    "Functional transformers are reusable and you can create many complicated things with them (think about Lego blocks).\n",
    "\n",
    "Assumptions\n",
    "-------------------\n",
    "\n",
    "1. We will be using scikit-learn interface to pipelines.\n",
    "2. We will use pandas dataframes as inputs to pipelines (useful).\n",
    "\n",
    "There are 2 types of building blocks of machine learning pipelines: transformers and estimators\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformers\n",
    "---------\n",
    "\n",
    "Blocks that have input and output and can be chained with other transformers.\n",
    "\n",
    "For example\n",
    "\n",
    "```\n",
    "Data -> [ Select variables ] -> [ Normalize ] -> [ Reduce dimensions ] -> Output\n",
    "```\n",
    "\n",
    "`[ Select variables ]` - transformer for selecting variables\n",
    "\n",
    "`[ Normalize ]` - normalization step\n",
    "\n",
    "`[ Reduce dimensions ]` - dimension reduction\n",
    "\n",
    "\n",
    "-------------------\n",
    "\n",
    "Because every transformer has the same type of data as input and output altogether they \n",
    "also form a transformer.\n",
    "\n",
    "```\n",
    "Input -> [ [ Select variables ] -> [ Normalize ] -> [ Reduce dimensions ] ] -> Output\n",
    "\n",
    "Input -> [               Data preprocessing transformation                ] -> Output\n",
    "```\n",
    "\n",
    "-------------------\n",
    "\n",
    "An example of transformer that does nothing\n",
    "\n",
    "```python\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class LazyTransformer(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def fit(self, x, y = None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, x):\n",
    "        return x\n",
    "```\n",
    "\n",
    "-------------------\n",
    "\n",
    "Notice that there are 2 methods:\n",
    "\n",
    "1. **fit** - learns the information about the data - it becomes a stateful transformer\n",
    "2. **transform** - applies the transformation \n",
    "\n",
    "There are 2 types of transformers:\n",
    "1. **stateful** - they learn something when calling fit method\n",
    "2. **stateless** - they don't learn anything\n",
    "\n",
    "**Why stateless transformers are useful?**\n",
    "\n",
    "Transformers that don't need historical data to learn can be used in a type of learning\n",
    "called `online learning`. This type of learning fits pipelines beacuse it is an algorithm\n",
    "that uses the stream of observations to learn.\n",
    "\n",
    "It doesn't keep the history so there would be no way to use stateful transformers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise\n",
    "--------------\n",
    "\n",
    "1. Write a transformer that adds some number to the input, the number that is added should be passed in `__init__`\n",
    "2. Write a transformer that normalizes the input:\n",
    "   - in the fit method you must save the column means\n",
    "3. Combine these 2 transformers into a pipeline:\n",
    "   - hint: write a class that accepts list of transformers as argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator,TransformerMixin\n",
    "\n",
    "# answer - start\n",
    "\n",
    "# fill the classes with code\n",
    "class AdderTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self,add=0):\n",
    "        self.add = add\n",
    "\n",
    "    def fit(self, x, y = None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, x):\n",
    "        return x+self.add\n",
    "\n",
    "class MeanNormalizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, x, y = None):\n",
    "        self.mean = np.mean(x)\n",
    "        return self\n",
    "\n",
    "    def transform(self, x):\n",
    "        return x - self.mean\n",
    "\n",
    "class TransformerPipeline(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self,list_transformers):\n",
    "        self.list = list_transformers\n",
    "\n",
    "    def fit(self, x, y = None):\n",
    "        x_ = x.copy()\n",
    "        for trans in self.list:\n",
    "            trans.fit(x_)\n",
    "            x_ = trans.transform(x_)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, x):\n",
    "        x_ = x.copy()\n",
    "        for trans in self.list:\n",
    "            x_ = trans.transform(x_)\n",
    "        return x_\n",
    "\n",
    "# answer - end\n",
    "\n",
    "# tests\n",
    "X = np.ones((10,10))\n",
    "adder = AdderTransformer(add=1)\n",
    "assert np.all(adder.transform(X) == X + 1), \"Adder transformer wrong\"\n",
    "\n",
    "normalizer = MeanNormalizer()\n",
    "assert np.allclose(normalizer.fit_transform(X),np.zeros((10,10))), \"Mean normalizer wrong\"\n",
    "\n",
    "double_adder = TransformerPipeline([AdderTransformer(add=1), \n",
    "                                    AdderTransformer(add=2)])\n",
    "\n",
    "assert np.allclose(double_adder.transform(X), X+3), \"TransformerPipeline wrong\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Double click to see the solution**\n",
    "\n",
    "<div class='spoiler'>\n",
    "\n",
    "class AdderTransformer(TransformerMixin):\n",
    "    \n",
    "    def __init__(self, add=0):\n",
    "        self.add = add\n",
    "        \n",
    "    def fit(self, x, y = None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, x):\n",
    "        return x + self.add\n",
    "    \n",
    "class MeanNormalizer(TransformerMixin):\n",
    "    \n",
    "    def __init__(self, add=0):\n",
    "        self.add = add\n",
    "        \n",
    "    def fit(self, x, y = None):\n",
    "        self.means = x.mean(axis=0)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, x):\n",
    "        return x - self.means    \n",
    "    \n",
    "class TransformerPipeline(TransformerMixin):\n",
    "    \n",
    "    def __init__(self, transformers):\n",
    "        self.transformers = transformers\n",
    "        \n",
    "    def fit(self, x, y = None):\n",
    "        x_ = x.copy()\n",
    "        for transformer in self.transformers:\n",
    "            transformer.fit(x_)\n",
    "            x_ = transformer.transform(x_)\n",
    "        return self\n",
    "        \n",
    "    def transform(self, x):\n",
    "        x_ = x.copy()\n",
    "        for transformer in self.transformers:\n",
    "            x_ = transformer.transform(x_)\n",
    "        return x_\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-learn pipelines to the rescue\n",
    "-------------\n",
    "\n",
    "Fortunately scikit-learn provides a set of helpful functions to deal with pipelines.\n",
    "2 of them are the most important:\n",
    "\n",
    "1. `sklearn.pipeline.make_pipelines`\n",
    "\n",
    "    In our previous example we could define our transformer like this\n",
    "    \n",
    "```python\n",
    "adder_normalizer = make_pipeline(\n",
    "    AdderTransformer(add=10),\n",
    "    MeanNormalizer()\n",
    ")\n",
    "```\n",
    "\n",
    "2. `sklearn.pipeline.make_union`\n",
    "\n",
    "    Creates a union of transformers\n",
    "    \n",
    "    ```\n",
    "    \n",
    "             transformer 1\n",
    "           /               \\\n",
    "          /                 \\\n",
    "    input                     output\n",
    "          \\                 /    \n",
    "           \\               /\n",
    "             transformer 2\n",
    "             \n",
    "    ```\n",
    "             \n",
    "    It is useful when the dataset consists of several types of data that one must \n",
    "    deal with separately.\n",
    "\n",
    "\n",
    "Alternative way to define pipelines\n",
    "--------------\n",
    "\n",
    "```python\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "adder_normalizer = Pipeline([\n",
    "    ('adder', AdderTransformer(add=10)),\n",
    "    ('normalizer', MeanNormalizer()),    \n",
    "])\n",
    "\n",
    "print(adder_normalizer)\n",
    "\n",
    ">> Pipeline(steps=[('adder', <__main__.AdderTransformer object at 0x7f9387473750>), ('normalizer', <__main__.MeanNormalizer object at 0x7f9387137e50>)])\n",
    "```\n",
    "\n",
    "It is useful to name the steps because sometimes we want to control the steps from outside - for example when searching for parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Heterogenous data\n",
    "==========================\n",
    "\n",
    "Normally datasets are not matrices of numbers.\n",
    "In real life it will be a mix of:\n",
    "- categorical features\n",
    "- numerical features\n",
    "- dates\n",
    "- text data\n",
    "- with missing values / without missing values\n",
    "\n",
    "Still you must create 1 pipeline to process all these types of information.\n",
    "\n",
    "Possible transformations:\n",
    "- **categorical features**:\n",
    "    - one hot encoding - converting to binary values\n",
    "    - convert to numerical values - by using a hash of categorical variable\n",
    "    - target averaging - replace categorical feature with an average of the target\n",
    "    \n",
    "- **numerical features**:\n",
    "    - fill missing values\n",
    "    - create bins with ranges \n",
    "    - normalize, scale\n",
    "    \n",
    "- **text**\n",
    "    - use bag of words vectorization\n",
    "    - word2vec, sentence2vec\n",
    "\n",
    "- **dates**\n",
    "    - extract years, months, days, days of week\n",
    "\n",
    "With pipelines you can split data flow into "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimators\n",
    "----------\n",
    "\n",
    "Normally at the end of the pipeline there are estimators -> predictive algortihms:\n",
    "    \n",
    "For example\n",
    "\n",
    "```\n",
    "Data -> [ Select variables ] -> [ Normalize ] -> [ Reduce dimensions ] -> [ Linear Regression ] -> Prediction\n",
    "```\n",
    "\n",
    "or more generally\n",
    "\n",
    "```\n",
    "Data -> [ Data preprocessing ] -> [ Estimator ] -> Prediction\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
