{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example of a **production** model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```scala\n",
    "\n",
    "package com.growbots.spark_models\n",
    "\n",
    "import ml.dmlc.xgboost4j.scala.spark.XGBoostEstimator\n",
    "import org.apache.spark.ml.feature.{CountVectorizer, RegexTokenizer, VectorAssembler}\n",
    "import org.apache.spark.ml.linalg.DenseVector\n",
    "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.{DataFrame, SparkSession}\n",
    "import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics\n",
    "\n",
    "\n",
    "object DomainQualification {\n",
    "  val spark: SparkSession = create_spark_session()\n",
    "\n",
    "  def main(args: Array[String]) {\n",
    "\n",
    "    val toInt = udf[Int, String](_.toInt)\n",
    "    val toDouble = udf[Double, String](_.toDouble)\n",
    "\n",
    "    // load labeled data\n",
    "    val labeled_data_raw = load_labeled_dataset()\n",
    "    labeled_data_raw.createOrReplaceTempView(\"labeled_data\")\n",
    "    labeled_data_raw.printSchema()\n",
    "\n",
    "    println(\"Total domains %d\".format(labeled_data_raw.count()))\n",
    "\n",
    "    val homepages = spark.read.parquet(\"gs://ai-bucket/company_crawl/parts.parquet\")\n",
    "    homepages.createOrReplaceTempView(\"pages\")\n",
    "\n",
    "    val company_profiles = spark.read.parquet(\"gs://ai-bucket/profiles/raw_company_data.parquet\")\n",
    "    company_profiles.createOrReplaceTempView(\"company_profiles\")\n",
    "    company_profiles.printSchema()\n",
    "\n",
    "    val domains_labeled = spark.sql(\n",
    "      \"\"\"\n",
    "      SELECT company_profiles.domain,\n",
    "             MAX(labeled_data.label) as label,\n",
    "             MAX(COALESCE(company_profiles.description,'')) as description,\n",
    "             MAX(COALESCE(company_profiles.specialties,\"\")) as specialties,\n",
    "             MAX(COALESCE(pages.html,\"\")) as html,\n",
    "             MAX(COALESCE(company_profiles.industry,'')) as industry,\n",
    "             MAX(COALESCE(company_profiles.headquarters_address_raw,'')) as address\n",
    "      FROM company_profiles\n",
    "           INNER JOIN labeled_data ON company_profiles.domain = labeled_data.domain\n",
    "           LEFT JOIN pages ON pages.domain = labeled_data.domain\n",
    "      WHERE\n",
    "           (company_profiles.description is not NULL or pages.html is not NULL)\n",
    "      GROUP BY 1\n",
    "      \"\"\").cache\n",
    "\n",
    "    println(\"Total training examples %d\".format(domains_labeled.count()))\n",
    "\n",
    "    val (pipeline, model) = createPipeline()\n",
    "\n",
    "    println(\"Validating\")\n",
    "    val Array(domains_labeled_tr, domains_labeled_te) = domains_labeled.randomSplit(Array(0.6, 0.4))\n",
    "    val pipeline_model = pipeline.fit(domains_labeled_tr)\n",
    "    val preds = pipeline_model\n",
    "      .transform(domains_labeled_te)\n",
    "      .cache\n",
    "\n",
    "    val preds_labels = preds\n",
    "      .rdd.map(row => (row.getAs[Double](\"label\"), row.getAs[Seq[Float]](\"probabilities\")(0).toDouble))\n",
    "\n",
    "    val metrics = new BinaryClassificationMetrics(preds_labels)\n",
    "    println(\"Auc = \" + metrics.areaUnderROC)\n",
    "\n",
    "    println(\"Training on all data\")\n",
    "    val pipeline_model_full = pipeline.fit(domains_labeled)\n",
    "\n",
    "    println(\"Predicting new profiles\")\n",
    "    val domains_not_labeled = spark.sql(\n",
    "      \"\"\"\n",
    "      SELECT company_profiles.domain,\n",
    "             MAX(COALESCE(company_profiles.description,'')) as description,\n",
    "             MAX(COALESCE(company_profiles.specialties,\"\")) as specialties,\n",
    "             MAX(COALESCE(pages.html,\"\")) as html,\n",
    "             MAX(COALESCE(company_profiles.industry,'')) as industry,\n",
    "             MAX(COALESCE(company_profiles.headquarters_address_raw,'')) as address\n",
    "      FROM company_profiles\n",
    "           LEFT JOIN labeled_data ON company_profiles.domain = labeled_data.domain\n",
    "           LEFT JOIN pages ON pages.domain = labeled_data.domain\n",
    "      WHERE labeled_data.domain is NULL and\n",
    "            (company_profiles.description is not NULL or pages.html is not NULL)\n",
    "      GROUP BY 1\n",
    "      \"\"\").repartition(200)\n",
    "\n",
    "    pipeline_model_full.transform(domains_not_labeled)\n",
    "      .select(\"domain\", \"probabilities\")\n",
    "      .write.mode(\"overwrite\")\n",
    "      .parquet(\"gs://ai-bucket/notebooks-data/sdr/qualify_growbots_leads-v2/domains_not_labeled_with_probs_v2.parquet\")\n",
    "  }\n",
    "\n",
    "  def create_spark_session(): SparkSession = {\n",
    "    return SparkSession\n",
    "      .builder\n",
    "      .appName(\"Domain Qualification\")\n",
    "      .getOrCreate()\n",
    "  }\n",
    "\n",
    "  def load_labeled_dataset(): DataFrame = {\n",
    "    val path = \"gs://ai-bucket/notebooks-data/sdr/qualify_growbots_leads-v2/domains_qualified_v2.csv\"\n",
    "    val labeled_data_raw = spark.read.option(\"inferSchema\", \"true\").csv(path)\n",
    "      .withColumnRenamed(\"_c0\", \"domain\")\n",
    "      .withColumnRenamed(\"_c1\", \"label\").distinct()\n",
    "    val labeled_data_raw_2 = labeled_data_raw.withColumn(\"label\", labeled_data_raw.col(\"label\").cast(\"double\"))\n",
    "    return labeled_data_raw_2\n",
    "  }\n",
    "\n",
    "  def createPipeline(): (Pipeline, XGBoostEstimator) = {\n",
    "\n",
    "    val tokenizer1 = new RegexTokenizer()\n",
    "      .setToLowercase(true)\n",
    "      .setPattern(\"(?u)\\\\b\\\\w\\\\w+\\\\b\") // default scikit-learn\n",
    "      .setGaps(false)\n",
    "      .setInputCol(\"description\")\n",
    "      .setOutputCol(\"description_words\")\n",
    "\n",
    "    val hashingTF1 = new CountVectorizer()\n",
    "      .setMinDF(5)\n",
    "      .setInputCol(tokenizer1.getOutputCol)\n",
    "      .setOutputCol(\"description_tf\")\n",
    "\n",
    "    val tokenizer2 = new RegexTokenizer()\n",
    "      .setToLowercase(true)\n",
    "      .setPattern(\"(?u)\\\\b\\\\w\\\\w+\\\\b\") // default scikit-learn\n",
    "      .setGaps(false)\n",
    "      .setInputCol(\"html\")\n",
    "      .setOutputCol(\"html_words\")\n",
    "\n",
    "    val hashingTF2 = new CountVectorizer()\n",
    "      .setMinDF(5)\n",
    "      .setInputCol(tokenizer2.getOutputCol)\n",
    "      .setOutputCol(\"html_tf\")\n",
    "\n",
    "    val tokenizer3 = new RegexTokenizer()\n",
    "      .setToLowercase(true)\n",
    "      .setPattern(\"(?u)\\\\b\\\\w\\\\w+\\\\b\") // default scikit-learn\n",
    "      .setGaps(false)\n",
    "      .setInputCol(\"specialties\")\n",
    "      .setOutputCol(\"specialties_words\")\n",
    "\n",
    "    val hashingTF3 = new CountVectorizer()\n",
    "      .setMinDF(5)\n",
    "      .setInputCol(tokenizer3.getOutputCol)\n",
    "      .setOutputCol(\"specialties_tf\")\n",
    "\n",
    "    val tokenizer4 = new RegexTokenizer()\n",
    "      .setToLowercase(true)\n",
    "      .setPattern(\"(?u)\\\\b\\\\w\\\\w+\\\\b\") // default scikit-learn\n",
    "      .setGaps(false)\n",
    "      .setInputCol(\"address\")\n",
    "      .setOutputCol(\"address_words\")\n",
    "\n",
    "    val hashingTF4 = new CountVectorizer()\n",
    "      .setMinDF(5)\n",
    "      .setInputCol(tokenizer4.getOutputCol)\n",
    "      .setOutputCol(\"address_tf\")\n",
    "\n",
    "    val tokenizer5 = new RegexTokenizer()\n",
    "      .setToLowercase(true)\n",
    "      .setPattern(\"(?u)\\\\b\\\\w\\\\w+\\\\b\") // default scikit-learn\n",
    "      .setGaps(false)\n",
    "      .setInputCol(\"industry\")\n",
    "      .setOutputCol(\"industry_words\")\n",
    "\n",
    "    val hashingTF5 = new CountVectorizer()\n",
    "      .setMinDF(5)\n",
    "      .setInputCol(tokenizer5.getOutputCol)\n",
    "      .setOutputCol(\"industry_tf\")\n",
    "\n",
    "    val va = new VectorAssembler()\n",
    "      .setInputCols(Array(\"description_tf\", \"html_tf\", \"specialties_tf\", \"address_tf\", \"industry_tf\"))\n",
    "      .setOutputCol(\"features\")\n",
    "\n",
    "    val numRound = 50\n",
    "    val numWorkers = 4\n",
    "    val paramMap = List(\n",
    "      \"eta\" -> 0.1f,\n",
    "      \"max_depth\" -> 6,\n",
    "      \"min_child_weight\" -> 3.0,\n",
    "      \"subsample\" -> 1.0,\n",
    "      \"colsample_bytree\" -> 0.82,\n",
    "      \"colsample_bylevel\" -> 0.9,\n",
    "      \"base_score\" -> 0.005,\n",
    "      \"eval_metric\" -> \"auc\",\n",
    "      \"seed\" -> 49,\n",
    "      \"silent\" -> 1,\n",
    "      \"objective\" -> \"binary:logistic\").toMap\n",
    "\n",
    "    val model = new XGBoostEstimator(xgboostParams = paramMap, round = numRound, nWorkers = numWorkers)\n",
    "\n",
    "    val pipeline = new Pipeline()\n",
    "      .setStages(Array(tokenizer1, hashingTF1,\n",
    "        tokenizer2, hashingTF2,\n",
    "        tokenizer3, hashingTF3,\n",
    "        tokenizer4, hashingTF4,\n",
    "        tokenizer5, hashingTF5, va, model))\n",
    "\n",
    "    (pipeline, model)\n",
    "  }\n",
    "}\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lucene tokenizer\n",
    "\n",
    "```scala\n",
    "package com.growbots.nlp\n",
    "\n",
    "import org.apache.lucene.analysis.en.EnglishAnalyzer\n",
    "import org.apache.lucene.analysis.tokenattributes.CharTermAttribute\n",
    "import org.apache.spark.ml.UnaryTransformer\n",
    "import org.apache.spark.ml.feature.Tokenizer\n",
    "import org.apache.spark.ml.util.{DefaultParamsReadable, _}\n",
    "import org.apache.spark.sql.types.{ArrayType, DataType, StringType}\n",
    "\n",
    "import scala.collection.mutable.ArrayBuffer\n",
    "\n",
    "class LuceneTokenizer(override val uid: String)\n",
    "  extends UnaryTransformer[String, Seq[String], LuceneTokenizer] with DefaultParamsWritable {\n",
    "\n",
    "  def this() = this(Identifiable.randomUID(\"lucene_tok\"))\n",
    "\n",
    "  private def tokenize(text: String): Seq[String] = {\n",
    "    val analyzer = new EnglishAnalyzer()\n",
    "    val tokenStream = analyzer.tokenStream(\"contents\", text)\n",
    "\n",
    "    val term = tokenStream.addAttribute(classOf[CharTermAttribute])\n",
    "\n",
    "    tokenStream.reset()\n",
    "\n",
    "    var result = ArrayBuffer.empty[String]\n",
    "\n",
    "    while (tokenStream.incrementToken()) {\n",
    "      result += term.toString.toLowerCase\n",
    "    }\n",
    "\n",
    "    tokenStream.end()\n",
    "    tokenStream.close()\n",
    "    result\n",
    "  }\n",
    "\n",
    "  override protected def createTransformFunc: String => Seq[String] = tokenize\n",
    "\n",
    "  override protected def validateInputType(inputType: DataType): Unit = {\n",
    "    require(inputType == StringType, s\"Input type must be string type but got $inputType.\")\n",
    "  }\n",
    "\n",
    "  override protected def outputDataType: DataType = new ArrayType(StringType, true)\n",
    "}\n",
    "\n",
    "object LuceneTokenizer extends DefaultParamsReadable[Tokenizer] {\n",
    "  override def load(path: String): Tokenizer = super.load(path)\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
