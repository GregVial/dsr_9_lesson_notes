{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modeling pipelines\n",
    "=================\n",
    "\n",
    "Thinking about modeling as a series of transformations is really helpful.\n",
    "Pipelines and functional transformations are the cleanest way to preprocess the data.\n",
    "It has its roots in Category theory from mathematics.\n",
    "\n",
    "Functional transformers are reusable and you can create many complicated things with them (think about Lego blocks).\n",
    "\n",
    "Assumptions\n",
    "-------------------\n",
    "\n",
    "1. We will be using scikit-learn interface to pipelines.\n",
    "2. We will use pandas dataframes as inputs to pipelines (useful).\n",
    "\n",
    "There are 2 types of building blocks of machine learning pipelines: transformers and estimators\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformers\n",
    "---------\n",
    "\n",
    "Blocks that have input and output and can be chained with other transformers.\n",
    "\n",
    "For example\n",
    "\n",
    "```\n",
    "Data -> [ Select variables ] -> [ Normalize ] -> [ Reduce dimensions ] -> Output\n",
    "```\n",
    "\n",
    "`[ Select variables ]` - transformer for selecting variables\n",
    "\n",
    "`[ Normalize ]` - normalization step\n",
    "\n",
    "`[ Reduce dimensions ]` - dimension reduction\n",
    "\n",
    "\n",
    "-------------------\n",
    "\n",
    "Because every transformer has the same type of data as input and output altogether they \n",
    "also form a transformer.\n",
    "\n",
    "```\n",
    "Input -> [ [ Select variables ] -> [ Normalize ] -> [ Reduce dimensions ] ] -> Output\n",
    "\n",
    "Input -> [               Data preprocessing transformation                ] -> Output\n",
    "```\n",
    "\n",
    "-------------------\n",
    "\n",
    "An example of transformer that does nothing\n",
    "\n",
    "```python\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class LazyTransformer(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def fit(self, x, y = None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, x):\n",
    "        return x\n",
    "```\n",
    "\n",
    "-------------------\n",
    "\n",
    "Notice that there are 2 methods:\n",
    "\n",
    "1. **fit** - learns the information about the data - it becomes a stateful transformer\n",
    "2. **transform** - applies the transformation \n",
    "\n",
    "There are 2 types of transformers:\n",
    "1. **stateful** - they learn something when calling fit method\n",
    "2. **stateless** - they don't learn anything"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise\n",
    "--------------\n",
    "\n",
    "1. Write a transformer that adds some number to the input\n",
    "2. Write a transformer that normalizes the input:\n",
    "   - in the fit method you must save the column means\n",
    "3. Combine these 2 transformers into a pipeline:\n",
    "   - hint: write a class that accepts list of transformers as argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.base import TransformerMixin\n",
    "\n",
    "\n",
    "class AdderTransformer(TransformerMixin):\n",
    "    \n",
    "    def __init__(self, add=0):\n",
    "        self.add = add\n",
    "        \n",
    "    def fit(self, x, y = None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, x):\n",
    "        return x + add\n",
    "    \n",
    "class MeanNormalizer(TransformerMixin):\n",
    "    \n",
    "    def __init__(self, add=0):\n",
    "        self.add = add\n",
    "        \n",
    "    def fit(self, x, y = None):\n",
    "        self.means = x.mean(axis=0)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, x):\n",
    "        return x - self.means    \n",
    "    \n",
    "class TransformerPipeline(TransformerMixin):\n",
    "    \n",
    "    def __init__(self, transformers):\n",
    "        self.transformers = transformers\n",
    "        \n",
    "    def fit(self, x, y = None):\n",
    "        x_ = x.copy()\n",
    "        for transformer in self.transformers:\n",
    "            transformer.fit(x_)\n",
    "            x_ = self.transformer.transform(x_)\n",
    "        return self\n",
    "        \n",
    "    def transform(self, x):\n",
    "        x_ = x.copy()\n",
    "        for transformer in self.transformers:\n",
    "            x_ = transformer.transform(x_)\n",
    "        return x_\n",
    "    \n",
    "#TODO: check if works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-learn pipelines to the rescue\n",
    "-------------\n",
    "\n",
    "Fortunately scikit-learn provides a set of helpful functions to deal with pipelines.\n",
    "2 of them are the most important:\n",
    "\n",
    "1. `sklearn.pipeline.make_pipelines`\n",
    "\n",
    "    In our previous example we could define our transformer like this\n",
    "    \n",
    "```python\n",
    "adder_normalizer = make_pipeline(\n",
    "    AdderTransformer(add=10),\n",
    "    MeanNormalizer()\n",
    ")\n",
    "```\n",
    "\n",
    "2. `sklearn.pipeline.make_union`\n",
    "\n",
    "    Creates a union of transformers\n",
    "    \n",
    "    ```\n",
    "    \n",
    "             transformer 1\n",
    "           /               \\\n",
    "          /                 \\\n",
    "    input                     output\n",
    "          \\                 /    \n",
    "           \\               /\n",
    "             transformer 2\n",
    "             \n",
    "    ```\n",
    "             \n",
    "    It is useful when the dataset consists of several types of data that one must \n",
    "    deal with separately.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Heterogenous data\n",
    "==========================\n",
    "\n",
    "Normally datasets are not matrices of numbers.\n",
    "In real life it will be a mix of:\n",
    "- categorical features\n",
    "- numerical features\n",
    "- dates\n",
    "- text data\n",
    "- with missing values / without missing values\n",
    "\n",
    "Still you must create 1 pipeline to process all these types of information.\n",
    "\n",
    "Possible transformations:\n",
    "- **categorical features**:\n",
    "    - one hot encoding - converting to binary values\n",
    "    - convert to numerical values - by using a hash of categorical variable\n",
    "    - target averaging - replace categorical feature with an average of the target\n",
    "    \n",
    "- **numerical features**:\n",
    "    - fill missing values\n",
    "    - create bins with ranges \n",
    "    - normalize, scale\n",
    "    \n",
    "- **text**\n",
    "    - use bag of words vectorization\n",
    "    - word2vec, sentence2vec\n",
    "\n",
    "- **dates**\n",
    "    - extract years, months, days, days of week\n",
    "\n",
    "With pipelines you can split data flow into "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimators\n",
    "----------\n",
    "\n",
    "Normally at the end of the pipeline there are estimators -> predictive algortihms:\n",
    "    \n",
    "For example\n",
    "\n",
    "```\n",
    "Data -> [ Select variables ] -> [ Normalize ] -> [ Reduce dimensions ] -> [ Linear Regression ] -> Prediction\n",
    "```\n",
    "\n",
    "or more generally\n",
    "\n",
    "```\n",
    "Data -> [ Data preprocessing ] -> [ Estimator ] -> Prediction\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Production example of a model\n",
    "---------------------------\n",
    "\n",
    "```python\n",
    "\n",
    "from string import lower\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import make_pipeline, make_union\n",
    "\n",
    "from some_library import ChangeCounter, TextStats, LanguageModelScore\n",
    "from some_library import (ApplyFunction, MissingValuesFiller,\n",
    "                          PandasSelector, make_pandas_categorical_vectorizer)\n",
    "from some_library import Densify\n",
    "\n",
    "\"\"\"\n",
    "Data preparation and validation:\n",
    "1. Put selector as a first transformer to make meaningful errors during\n",
    "   calling\n",
    "2. Fill missing values\n",
    "3. Convert src_lang and dst_lang to lowercase\n",
    "\n",
    "Features:\n",
    "1. Character ngrams of src_text and dst_text\n",
    "2. Word ngrams (with high frequency) of src_text and dst_text\n",
    "3. TextStats - many different measures to compare src_text and dst_text\n",
    "4. One hot encoding of categorical features: src_lang, dst_lang, category\n",
    "\n",
    "Model:\n",
    "1. RandomForestsClassifier\n",
    "\"\"\"\n",
    "\n",
    "classifier = make_pipeline(\n",
    "    PandasSelector(columns=['category', 'src_lang', 'dst_lang',\n",
    "                            'src_text', 'dst_text']),\n",
    "    MissingValuesFiller(),\n",
    "    ApplyFunction(columns=['src_lang', 'dst_lang'], fun=lower),\n",
    "\n",
    "    # here we start adding features\n",
    "    make_union(\n",
    "        make_pipeline(\n",
    "            PandasSelector(columns=['src_text']),\n",
    "            CountVectorizer(analyzer='char',\n",
    "                            ngram_range=(1, 1),\n",
    "                            min_df=10)\n",
    "        ),\n",
    "        make_pipeline(\n",
    "            PandasSelector(columns=['dst_text']),\n",
    "            CountVectorizer(analyzer='char',\n",
    "                            ngram_range=(1, 1),\n",
    "                            min_df=10)\n",
    "        ),\n",
    "        make_pipeline(\n",
    "            PandasSelector(columns=['src_text']),\n",
    "            CountVectorizer(analyzer='word',\n",
    "                            ngram_range=(1, 1),\n",
    "                            min_df=25)\n",
    "        ),\n",
    "        make_pipeline(\n",
    "            PandasSelector(columns=['dst_text']),\n",
    "            CountVectorizer(analyzer='word',\n",
    "                            ngram_range=(1, 1),\n",
    "                            min_df=25)\n",
    "        ),\n",
    "        make_pandas_categorical_vectorizer(\n",
    "            columns=['src_lang', 'dst_lang', 'category']\n",
    "        ),\n",
    "        TextStats()\n",
    "    ),\n",
    "\n",
    "    # densify makes RandomForestClassifier much faster\n",
    "    Densify(),\n",
    "    RandomForestClassifier(\n",
    "        n_estimators=100,\n",
    "        n_jobs=-1,\n",
    "        min_samples_split=20, min_samples_leaf=10,\n",
    "        verbose=True,\n",
    "        random_state=1)\n",
    ")```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
