{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>/*@font-face {\n",
       "\tfont-family: \"Computer Modern\";\n",
       "\tsrc: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunss.otf');\n",
       "}\n",
       "@font-face {\n",
       "\tfont-family: \"Computer Modern\";\n",
       "\tfont-weight: bold;\n",
       "\tsrc: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunsx.otf');\n",
       "}\n",
       "@font-face {\n",
       "\tfont-family: \"Computer Modern\";\n",
       "\tfont-style: oblique;\n",
       "\tsrc: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunsi.otf');\n",
       "}\n",
       "@font-face {\n",
       "\tfont-family: \"Computer Modern\";\n",
       "\tfont-weight: bold;\n",
       "\tfont-style: oblique;\n",
       "\tsrc: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunso.otf');\n",
       "}*/\n",
       "\n",
       ".navbar-brand, .current_kernel_logo {display:none}\n",
       ".container {\n",
       "    width:80%;    \n",
       "}\n",
       "\n",
       "h1 {\n",
       "\tfont-family: Helvetica, serif;\n",
       "}\n",
       "h4{\n",
       "\tmargin-top:12px;\n",
       "\tmargin-bottom: 3px;\n",
       "   }\n",
       "div.text_cell_render{\n",
       "\tfont-family: Computer Modern, \"Helvetica Neue\", Arial, Helvetica, Geneva, sans-serif;\n",
       "\tline-height: 145%;\n",
       "\tfont-size: 100%;\n",
       "\twidth:100%;\n",
       "\tmargin-left:auto;\n",
       "\tmargin-right:auto;\n",
       "}\n",
       ".CodeMirror{\n",
       "\t\tfont-family: \"Source Code Pro\", source-code-pro,Consolas, monospace;\n",
       "}\n",
       ".prompt{\n",
       "\tdisplay: None;\n",
       "}\n",
       ".text_cell_render h5 {\n",
       "\tfont-weight: 300;\n",
       "\tfont-size: 22pt;\n",
       "\t/*color: #4057A1;*/\n",
       "\tfont-style: italic;\n",
       "\tmargin-bottom: .5em;\n",
       "\tmargin-top: 0.5em;\n",
       "\tdisplay: block;\n",
       "}\n",
       "\n",
       ".warning{\n",
       "\tcolor: rgb( 240, 20, 20 )\n",
       "\t}   \n",
       "\n",
       "div.spoiler {\n",
       "\tdisplay: none;\n",
       "}\n",
       "\n",
       ".rendered_html code {\n",
       "\tborder: 0;\n",
       "\t/*background-color: #eee;*/\n",
       "\tfont-size: 100%;\n",
       "\tpadding: 1px 2px;\n",
       "}\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import css_from_file\n",
    "css_from_file('style/style.css')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refactoring\n",
    "----------------\n",
    "\n",
    "Kaggle provides a functionality for running the scripts. Below there is an example of a script that achieves a very good score on the leaderboard. It is simple and easy to follow but it is not very complex. \n",
    "\n",
    "https://www.kaggle.com/tunguz/homesite-quote-conversion/xgboost-benchmark-1/run/102736/code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import preprocessing\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "seed = 666\n",
    "\n",
    "train = pd.read_csv('data/homesite/train.csv.zip')\n",
    "test = pd.read_csv('data/homesite/test.csv.zip')\n",
    "\n",
    "y = train.QuoteConversion_Flag.values\n",
    "train = train.drop(['QuoteNumber', 'QuoteConversion_Flag'], axis=1)\n",
    "test = test.drop('QuoteNumber', axis=1)\n",
    "\n",
    "# Lets play with some dates\n",
    "train['Date'] = pd.to_datetime(pd.Series(train['Original_Quote_Date']))\n",
    "train = train.drop('Original_Quote_Date', axis=1)\n",
    "\n",
    "test['Date'] = pd.to_datetime(pd.Series(test['Original_Quote_Date']))\n",
    "test = test.drop('Original_Quote_Date', axis=1)\n",
    "\n",
    "train['Year'] = train['Date'].apply(lambda x: int(str(x)[:4]))\n",
    "train['Month'] = train['Date'].apply(lambda x: int(str(x)[5:7]))\n",
    "train['weekday'] = train['Date'].dt.dayofweek\n",
    "\n",
    "\n",
    "test['Year'] = test['Date'].apply(lambda x: int(str(x)[:4]))\n",
    "test['Month'] = test['Date'].apply(lambda x: int(str(x)[5:7]))\n",
    "test['weekday'] = test['Date'].dt.dayofweek\n",
    "\n",
    "train = train.drop('Date', axis=1)\n",
    "test = test.drop('Date', axis=1)\n",
    "\n",
    "train = train.fillna(-1)\n",
    "test = test.fillna(-1)\n",
    "\n",
    "for f in train.columns:\n",
    "    if train[f].dtype=='object':\n",
    "        print(f)\n",
    "        lbl = preprocessing.LabelEncoder()\n",
    "        lbl.fit(list(train[f].values) + list(test[f].values))\n",
    "        train[f] = lbl.transform(list(train[f].values))\n",
    "        test[f] = lbl.transform(list(test[f].values))\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=100,\n",
    "                             n_jobs=-1)\n",
    "                        \n",
    "xgb_model = clf.fit(train, y, eval_metric=\"auc\")\n",
    "\n",
    "preds = clf.predict_proba(test)[:,1]\n",
    "sample = pd.read_csv('../input/sample_submission.csv')\n",
    "sample.QuoteConversion_Flag = preds\n",
    "sample.to_csv('xgb_benchmark.csv', index=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "1. What are the building blocks of this pipeline. What transformations you must do to go from raw data to preprocessed data you can use in the Machine Learning algorithm\n",
    "\n",
    "2. Do you need to process the training and testing data at the same time? What if you didn't have testing data when preprocessing the data? \n",
    "\n",
    "3. Do you see some duplicated code?\n",
    "\n",
    "4. Rewrite this code as a chain of transformers in the spirit of the previous examples.\n",
    "\n",
    "5. Process the training data, save the serialized pipeline (using `joblib.dump`), load the pipeline (using `joblib.load`) and predict testing data. \n",
    "\n",
    "Hint: use `PandasSelector` from previous exercise (you can import an improved version from `pandas_transformers.py` file)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
