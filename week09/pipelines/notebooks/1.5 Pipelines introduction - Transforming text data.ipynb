{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>/*@font-face {\n",
       "\tfont-family: \"Computer Modern\";\n",
       "\tsrc: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunss.otf');\n",
       "}\n",
       "@font-face {\n",
       "\tfont-family: \"Computer Modern\";\n",
       "\tfont-weight: bold;\n",
       "\tsrc: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunsx.otf');\n",
       "}\n",
       "@font-face {\n",
       "\tfont-family: \"Computer Modern\";\n",
       "\tfont-style: oblique;\n",
       "\tsrc: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunsi.otf');\n",
       "}\n",
       "@font-face {\n",
       "\tfont-family: \"Computer Modern\";\n",
       "\tfont-weight: bold;\n",
       "\tfont-style: oblique;\n",
       "\tsrc: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunso.otf');\n",
       "}*/\n",
       "\n",
       ".navbar-brand, .current_kernel_logo {display:none}\n",
       ".container {\n",
       "    width:80%;    \n",
       "}\n",
       "\n",
       "h1 {\n",
       "\tfont-family: Helvetica, serif;\n",
       "}\n",
       "h4{\n",
       "\tmargin-top:12px;\n",
       "\tmargin-bottom: 3px;\n",
       "   }\n",
       "div.text_cell_render{\n",
       "\tfont-family: Computer Modern, \"Helvetica Neue\", Arial, Helvetica, Geneva, sans-serif;\n",
       "\tline-height: 145%;\n",
       "\tfont-size: 100%;\n",
       "\twidth:100%;\n",
       "\tmargin-left:auto;\n",
       "\tmargin-right:auto;\n",
       "}\n",
       ".CodeMirror{\n",
       "\t\tfont-family: \"Source Code Pro\", source-code-pro,Consolas, monospace;\n",
       "}\n",
       ".text_cell_render h5 {\n",
       "\tfont-weight: 300;\n",
       "\tfont-size: 22pt;\n",
       "\t/*color: #4057A1;*/\n",
       "\tfont-style: italic;\n",
       "\tmargin-bottom: .5em;\n",
       "\tmargin-top: 0.5em;\n",
       "\tdisplay: block;\n",
       "}\n",
       "\n",
       ".warning{\n",
       "\tcolor: rgb( 240, 20, 20 )\n",
       "\t}   \n",
       "\n",
       "div.spoiler {\n",
       "\tdisplay: none;\n",
       "}\n",
       "\n",
       ".rendered_html code {\n",
       "\tborder: 0;\n",
       "\t/*background-color: #eee;*/\n",
       "\tfont-size: 100%;\n",
       "\tpadding: 1px 2px;\n",
       "}\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import css_from_file\n",
    "css_from_file('style/style.css')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import (\n",
    "    CountVectorizer, TfidfVectorizer, HashingVectorizer\n",
    ")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read csv file into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>category_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>170255</th>\n",
       "      <td>Maison Martin Margiela fabric high necklace wi...</td>\n",
       "      <td>Jewellery &amp; Watches</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440097</th>\n",
       "      <td>PERSONALISED Wedding Engagement Anniversary Pa...</td>\n",
       "      <td>Crafts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412988</th>\n",
       "      <td>Tax Disc Holder Tube suitable for Triumph Adve...</td>\n",
       "      <td>Vehicle Parts &amp; Accessories</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>739040</th>\n",
       "      <td>Aromatherapy Hand &amp; Nail Cream Essential Oil 6...</td>\n",
       "      <td>Health &amp; Beauty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211177</th>\n",
       "      <td>New Genuine BATTERY EB425161LU 1500mAh FOR SAM...</td>\n",
       "      <td>Mobile Phones &amp; Communication</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    title  \\\n",
       "170255  Maison Martin Margiela fabric high necklace wi...   \n",
       "440097  PERSONALISED Wedding Engagement Anniversary Pa...   \n",
       "412988  Tax Disc Holder Tube suitable for Triumph Adve...   \n",
       "739040  Aromatherapy Hand & Nail Cream Essential Oil 6...   \n",
       "211177  New Genuine BATTERY EB425161LU 1500mAh FOR SAM...   \n",
       "\n",
       "                        category_name  \n",
       "170255            Jewellery & Watches  \n",
       "440097                         Crafts  \n",
       "412988    Vehicle Parts & Accessories  \n",
       "739040                Health & Beauty  \n",
       "211177  Mobile Phones & Communication  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/ebaytitles.csv\")\n",
    "df = df.sample(frac=0.1) # delete this line if you are brave and have many GBs of RAM\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print out unique values of a column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Jewellery & Watches', 'Crafts', 'Vehicle Parts & Accessories',\n",
       "       'Health & Beauty', 'Mobile Phones & Communication',\n",
       "       'Clothes, Shoes & Accessories', 'Home, Furniture & DIY',\n",
       "       'Sporting Goods', 'Pet Supplies', 'Computers/Tablets & Networking',\n",
       "       'Art', 'Toys & Games', 'Business, Office & Industrial',\n",
       "       'Books, Comics & Magazines', 'Music', 'DVDs, Films & TV',\n",
       "       'Cameras & Photography', 'Sports Memorabilia', 'Sound & Vision',\n",
       "       'Collectibles', 'Musical Instruments & Gear', 'Baby',\n",
       "       'Consumer Electronics', 'Garden & Patio', 'Dolls & Bears',\n",
       "       'Antiques', 'Stamps', 'Cell Phones & Accessories',\n",
       "       'Everything Else', 'Coins & Paper Money', 'Video Games & Consoles',\n",
       "       'Pottery, Porcelain & Glass', 'Wholesale & Job Lots',\n",
       "       'Entertainment Memorabilia', 'Travel', 'Holidays & Travel'], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.category_name.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into train and test observations - there is a column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = df.title.values\n",
    "y = df.category_name.values\n",
    "\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X, \n",
    "                                          y,\n",
    "                                          test_size=0.1,\n",
    "                                          random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise \n",
    "------------------\n",
    "\n",
    "1. Count how many titles are in each category (```pandas.DataFrame.groupby```). Print out most common at the top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "category_name\n",
       "Vehicle Parts & Accessories       23412\n",
       "Clothes, Shoes & Accessories      16819\n",
       "Home, Furniture & DIY             12812\n",
       "Computers/Tablets & Networking     6755\n",
       "Jewellery & Watches                6216\n",
       "Sporting Goods                     4688\n",
       "Mobile Phones & Communication      3858\n",
       "Crafts                             3371\n",
       "Health & Beauty                    3292\n",
       "Toys & Games                       2946\n",
       "Business, Office & Industrial      2839\n",
       "Collectibles                       2338\n",
       "Sound & Vision                     1902\n",
       "Music                              1324\n",
       "Garden & Patio                     1031\n",
       "Cameras & Photography               904\n",
       "Baby                                700\n",
       "Pet Supplies                        633\n",
       "Art                                 598\n",
       "DVDs, Films & TV                    586\n",
       "Video Games & Consoles              486\n",
       "Books, Comics & Magazines           463\n",
       "Musical Instruments & Gear          425\n",
       "Dolls & Bears                       281\n",
       "Sports Memorabilia                  270\n",
       "Coins & Paper Money                 232\n",
       "Antiques                            197\n",
       "Everything Else                     140\n",
       "Consumer Electronics                132\n",
       "Pottery, Porcelain & Glass          100\n",
       "Wholesale & Job Lots                 93\n",
       "Stamps                               88\n",
       "Cell Phones & Accessories            60\n",
       "Entertainment Memorabilia             6\n",
       "Travel                                2\n",
       "Holidays & Travel                     1\n",
       "Name: title, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##########################\n",
    "# put your solution here #\n",
    "##########################\n",
    "\n",
    "grouped = df.groupby(by=\"category_name\")['title'].count()\n",
    "grouped = grouped.sort_values(ascending=False)\n",
    "grouped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a>Double click to show the solution</a>\n",
    "<div class='spoiler'>\n",
    "\n",
    "frequencies = df.groupby(\"category_name\")[\"title\"].count()\n",
    "frequencies.sort_values(inplace=True,ascending=False)\n",
    "print(frequencies)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag of words\n",
    "--------------------\n",
    "\n",
    "Different types of vectorizers:\n",
    "\n",
    "<ul>\n",
    "<li>```sklearn.feature_extraction.text.CountVectorizer``` - Counts the number of times a word appears in the text</li>\n",
    "<li>```sklearn.feature_extraction.text.TfidfVectorizer``` - Weighs the words according to the importance of the word in the context of whole collection. Is the word ```the``` important if it appears in all documents?</li>\n",
    "<li>```sklearn.feature_extraction.text.HashingVectorizer``` - Useful when you don't know the vocabulary upfront. Feature number is calculated as ```hash(token) % vocabulary_size```.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise\n",
    "-------------------\n",
    "1. Use ```CountVectorizer``` / ```TfidfVectorizer``` to fit the collection of documents\n",
    "2. How many unique tokens are there in text? Print some examples (ie first few hundred).\n",
    "3. What methods you can use to reduce this number? \n",
    "   - Check out and experiment with the arguments: ```ngram_range```, ```min_df```. How the vocabulary size changes with each change?\n",
    "   - What would you replace / delete from the text?\n",
    "4. Write a custom function `clean_text` that accepts a text as input and transforms it (remove/hash numbers, delete short/long words etc.)\n",
    "5. (Extra points) When would you use ```HashingVectorizer```?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['coco', 'l9338', 'ladies', 'black', 'flat', 'knee', 'high', 'pull', 'on', 'boots', 'sale', 'cisco', 'male', 'db', '60', 'to', '15', 'meter', '10', 'cable', '72', '0789', '01', 'cab', 'x21mt', 'hp', '650', 'usb', 'board', '01016yy00', '600', 'inspector', 'parker', 'betrapped', 'bundle', 'pc', 'cd', 'brand', 'new', 'sealed', 'oem', 'box', 'hotpoint', 'wmpg762guk', 'washing', 'machine', 'drum', 'paddle', 'hole', 'rug']\n",
      "71706\n",
      "['coco', 'l9338', 'ladies', 'black', 'flat', 'knee', 'high', 'pull', 'on', 'boots', 'sale', 'cisco', 'male', 'db', '60', 'to', '15', 'meter', '10', 'cable', '72', '0789', '01', 'cab', 'x21mt', 'hp', '650', 'usb', 'board', '01016yy00', '600', 'inspector', 'parker', 'betrapped', 'bundle', 'pc', 'cd', 'brand', 'new', 'sealed', 'oem', 'box', 'hotpoint', 'wmpg762guk', 'washing', 'machine', 'drum', 'paddle', 'hole', 'rug']\n",
      "71706\n"
     ]
    }
   ],
   "source": [
    "##########################\n",
    "# put your solution here #\n",
    "##########################\n",
    "\n",
    "cv = CountVectorizer()\n",
    "cv.fit(X_tr)\n",
    "print(list(cv.vocabulary_)[:50])\n",
    "print(len(cv.vocabulary_))\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "tfidf.fit(X_tr)\n",
    "print(list(tfidf.vocabulary_)[:50])\n",
    "print(len(tfidf.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['coco', 'l9338', 'ladies', 'black', 'flat', 'knee', 'high', 'pull', 'on', 'boots', 'sale', 'cisco', 'male', 'db', '60', 'to', '15', 'meter', '10', 'cable', '72', '01', 'cab', 'hp', '650', 'usb', 'board', '600', 'inspector', 'parker', 'bundle', 'pc', 'cd', 'brand', 'new', 'sealed', 'oem', 'box', 'hotpoint', 'wmpg762guk', 'washing', 'machine', 'drum', 'paddle', 'hole', 'rug', 'shaggy', 'thick', 'luxury', 'pile']\n",
      "29244\n"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer(min_df=2)\n",
    "cv.fit(X_tr)\n",
    "print(list(cv.vocabulary_)[:50])\n",
    "print(len(cv.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['coco', 'l9338', 'ladies', 'black', 'flat', 'knee', 'high', 'pull', 'on', 'boots', 'sale', 'coco l9338', 'l9338 ladies', 'ladies black', 'black flat', 'flat knee', 'knee high', 'high pull', 'pull on', 'on boots', 'boots sale', 'cisco', 'male', 'db', '60', 'to', '15', 'meter', '10', 'cable', '72', '0789', '01', 'cab', 'x21mt', 'cisco male', 'male db', 'db 60', '60 to', 'to male', 'db 15', '15 meter', 'meter 10', '10 cable', 'cable 72', '72 0789', '0789 01', '01 cab', 'cab x21mt', 'hp']\n",
      "477233\n"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer(ngram_range=(1,2))\n",
    "cv.fit(X_tr)\n",
    "print(list(cv.vocabulary_)[:50])\n",
    "print(len(cv.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Coco', 'Ladies', 'Black', 'Flat', 'Knee', 'High', 'Pull', 'On', 'Boots', 'Sale', 'Cisco', 'Male', 'DB', 'to', 'Meter', 'Cable', 'CAB', 'MT', 'HP', 'USB', 'Board', 'Inspector', 'Parker', 'Betrapped', 'Bundle', 'PC', 'CD', 'BRAND', 'NEW', 'SEALED', 'OEM', 'BOX', 'Hotpoint', 'GUK', 'Washing', 'Machine', 'Drum', 'Paddle', 'Hole', 'RUG', 'SHAGGY', 'THICK', 'LUXURY', 'PILE', 'SILVER', 'GREY', 'CM', 'New', 'NGK', 'Ignition']\n",
      "56198\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(txt):\n",
    "    txt_ = txt\n",
    "    txt_ = re.sub(r'[A-Za-z]+[0-9]+',r'',txt_)\n",
    "    txt_ = re.sub(r'[0-9]+',r'',txt_)\n",
    "    return txt_\n",
    "\n",
    "sample=\"Greg01 00034234 023F FF\"\n",
    "clean_text(sample)\n",
    "\n",
    "cv = CountVectorizer(preprocessor=clean_text)\n",
    "cv.fit(X_tr)\n",
    "print(list(cv.vocabulary_)[:50])\n",
    "print(len(cv.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a>Double click to show the solution</a>\n",
    "<div class='spoiler'>\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import re\n",
    "\n",
    "def clean_text(t):\n",
    "    t = t.lower()\n",
    "    t = re.sub(\"[^A-Za-z0-9]\",\" \",t)\n",
    "    t = re.sub(\"[0-9]+\",\"#\",t)\n",
    "    return t\n",
    "\n",
    "vectorizers = [\n",
    "     (\"vanilla\",\n",
    "          CountVectorizer())\n",
    "    ,(\"preprocessing\",\n",
    "          CountVectorizer(preprocessor=clean_text))\n",
    "    ,(\"preprocessing + min_df=10\",\n",
    "          CountVectorizer(preprocessor=clean_text,\n",
    "                          min_df=10))\n",
    "]\n",
    "\n",
    "for vect_name, vect in vectorizers:\n",
    "    print(vect_name)\n",
    "    vect.fit(X_tr)\n",
    "    \n",
    "    print(list(vect.vocabulary_)[:10])\n",
    "    print(len(vect.vocabulary_))\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming\n",
    "------------------\n",
    "\n",
    "Linguistic normalization in which variant forms are reduced to a common form\n",
    "\n",
    "    connection\n",
    "    connections\n",
    "    connective     --->   connect\n",
    "    connected\n",
    "    connecting\n",
    "    \n",
    "Usage:\n",
    "\n",
    "    import snowballstemmer\n",
    "\n",
    "    stemmer = snowballstemmer.stemmer('english')\n",
    "    print(stemmer.stemWords(\"We are the world\".split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are the world\n"
     ]
    }
   ],
   "source": [
    "import snowballstemmer\n",
    "\n",
    "stemmer = snowballstemmer.stemmer('english')\n",
    "print(\" \".join(stemmer.stemWords(\"We are the worlds\".split())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**How to make preprocessing faster?**\n",
    "\n",
    "There is map a function.\n",
    "Below there is a comparison of three methods from the most naive to the fastest. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 908 ms, sys: 8 ms, total: 916 ms\n",
      "Wall time: 918 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "preprocessed_naive = X_tr[:]\n",
    "for n in range(preprocessed_naive.shape[0]):\n",
    "    preprocessed_naive[n] = clean_text(preprocessed_naive[n])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using ```map``` function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 16 Âµs\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "preprocessed_map = map(clean_text, X_tr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using ```pool.map``` function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 68 ms, sys: 124 ms, total: 192 ms\n",
      "Wall time: 375 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from multiprocessing import Pool\n",
    "\n",
    "pool = Pool(5)\n",
    "preprocessed_poolmap = pool.map(clean_text, X_tr)\n",
    "pool.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting it into a pipeline\n",
    "----------------------\n",
    "\n",
    "Now that we know how to transform text data, let's put it into a pipeline.\n",
    "\n",
    "1. Create a pipeline with `CountVectorizer`, `StandardScaler` and `SGDClassifier` as your final algorithm\n",
    "    a) use alternative format for pipeline definition when you name the steps - refer to the documentation how to do this\n",
    "2. Using ```sklearn.metrics.classification_report``` create a report about your classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\npreds = cross_val_predict(pipeline, \\n                          X_tr, \\n                          y_tr, \\n                          cv=8, n_jobs=-1, verbose=True)\\n                          '"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##########################\n",
    "# put your solution here #\n",
    "##########################\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "import snowballstemmer\n",
    "\n",
    "stemmer = snowballstemmer.stemmer('english')\n",
    "\n",
    "def clean_text(t):\n",
    "    t = t.lower()\n",
    "    t = re.sub(\"[^A-Za-z0-9]\",\" \",t)\n",
    "    t = re.sub(\"[0-9]+\",\"#\",t)\n",
    "    t = \" \".join(stemmer.stemWords(t.split()))\n",
    "    return t\n",
    "\n",
    "pipeline = Pipeline([\n",
    "                ('count', CountVectorizer(min_df=5,preprocessor=clean_text)),\n",
    "                ('scale', StandardScaler(with_mean=False)),\n",
    "                ('classify', SGDClassifier())\n",
    "            ])\n",
    "\"\"\"\n",
    "preds = cross_val_predict(pipeline, \n",
    "                          X_tr, \n",
    "                          y_tr, \n",
    "                          cv=8, n_jobs=-1, verbose=True)\n",
    "                          \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                precision    recall  f1-score   support\n",
      "\n",
      "                           Art       0.58      0.51      0.54       519\n",
      "                          Baby       0.69      0.56      0.62       640\n",
      "                         Music       0.89      0.85      0.87      1205\n",
      "                        Crafts       0.81      0.74      0.78      3040\n",
      "                        Stamps       0.58      0.70      0.63        80\n",
      "                        Travel       0.00      0.50      0.00         2\n",
      "                      Antiques       0.28      0.29      0.28       182\n",
      "                  Collectibles       0.70      0.60      0.65      2109\n",
      "                  Pet Supplies       0.76      0.62      0.68       573\n",
      "                  Toys & Games       0.79      0.68      0.73      2654\n",
      "                 Dolls & Bears       0.81      0.75      0.78       253\n",
      "                Garden & Patio       0.75      0.61      0.68       931\n",
      "                Sound & Vision       0.73      0.61      0.67      1726\n",
      "                Sporting Goods       0.79      0.68      0.73      4237\n",
      "               Everything Else       0.54      0.51      0.52       124\n",
      "               Health & Beauty       0.82      0.74      0.78      2957\n",
      "              DVDs, Films & TV       0.82      0.71      0.76       529\n",
      "             Holidays & Travel       0.00      0.00      0.00         1\n",
      "            Sports Memorabilia       0.60      0.52      0.56       239\n",
      "           Coins & Paper Money       0.84      0.72      0.78       210\n",
      "           Jewellery & Watches       0.94      0.90      0.92      5589\n",
      "          Consumer Electronics       0.49      0.42      0.45       117\n",
      "          Wholesale & Job Lots       0.41      0.51      0.45        85\n",
      "         Cameras & Photography       0.83      0.73      0.78       822\n",
      "         Home, Furniture & DIY       0.85      0.79      0.82     11550\n",
      "        Video Games & Consoles       0.80      0.72      0.76       436\n",
      "     Books, Comics & Magazines       0.75      0.63      0.69       412\n",
      "     Cell Phones & Accessories       0.20      0.22      0.21        54\n",
      "     Entertainment Memorabilia       0.00      0.00      0.00         5\n",
      "    Musical Instruments & Gear       0.63      0.49      0.55       383\n",
      "    Pottery, Porcelain & Glass       0.47      0.47      0.47        92\n",
      "   Vehicle Parts & Accessories       0.95      0.93      0.94     21021\n",
      "  Clothes, Shoes & Accessories       0.93      0.89      0.91     15096\n",
      " Business, Office & Industrial       0.64      0.53      0.58      2549\n",
      " Mobile Phones & Communication       0.88      0.84      0.86      3476\n",
      "Computers/Tablets & Networking       0.92      0.88      0.90      6102\n",
      "\n",
      "                   avg / total       0.87      0.81      0.84     90000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def add_padding(t):\n",
    "    return t.rjust(30)\n",
    "\n",
    "print(classification_report(list(map(add_padding, y_tr)), \n",
    "                            list(map(add_padding, preds))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a>Double click to show the solution</a>\n",
    "<div class='spoiler'>\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.cross_validation import cross_val_predict\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "clf = Pipeline([('vect', CountVectorizer(min_df=10, preprocessor=clean_text)),\n",
    "                ('scaling', StandardScaler(with_mean=False)),\n",
    "                ('clf', GridSearchCV(SGDClassifier(), param_grid={\"alpha\":[0.1,0.01,0.001]}))])\n",
    "\n",
    "preds = cross_val_predict(clf, \n",
    "                          X_tr, \n",
    "                          y_tr, \n",
    "                          cv=8, n_jobs=-1, verbose=True)\n",
    "\n",
    "def add_padding(t):\n",
    "    return t.rjust(30)\n",
    "\n",
    "print(classification_report(list(map(add_padding, y_tr)), \n",
    "                            list(map(add_padding, preds))))\n",
    "\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid search\n",
    "--------------------------\n",
    "\n",
    "Scikit-learn has `GridSearchCV` and `RandomizedSearchCV`. Both have the same functionality and can be used to find good parameters for the models. What is great about both these classes that they are both transformers - they return an estimator so you can chain them and put in your pipeline.\n",
    "\n",
    "**GridSearchCV** - you specify the exact values of the parameters you want to test\n",
    "**RandomizedSearchCV** - you specify ranges of parameters\n",
    "\n",
    "Exercise\n",
    "----------------------\n",
    "\n",
    "1. Use `GridSearchCV` or `RandomizedSearchCV` to find the best parameters for the models. Check at least 2 parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid search\n",
      "\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:  1.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.864 {'count__analyzer': 'char', 'count__binary': True, 'count__ngram_range': (1, 7)}\n"
     ]
    }
   ],
   "source": [
    "##########################\n",
    "# put your solution here #\n",
    "##########################\n",
    "\n",
    "from sklearn.grid_search import GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "print(\"Grid search\")\n",
    "print()\n",
    "\"\"\"\n",
    "params = {'count__ngram_range': [(1, 1), (1, 2), (1, 3), (1, 4)],\n",
    "          'count__analyzer': [\"word\",\"char\"],\n",
    "          'count__binary': [True, False]}\n",
    "\"\"\"\n",
    "params = {'count__ngram_range': [(1,7)],\n",
    "          'count__analyzer': [\"char\"],\n",
    "          'count__binary': [True]}\n",
    "\n",
    "grid_clf = GridSearchCV(pipeline, params, n_jobs=-1, verbose=True)\n",
    "grid_clf.fit(X_tr, y_tr)\n",
    "\n",
    "best_params = sorted(grid_clf.grid_scores_, key=lambda x: -x[1])\n",
    "\n",
    "for params, score, _ in best_params:\n",
    "    print(score, params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for params, score, _ in best_params:\n",
    "    print(score, params) \n",
    "    \n",
    "print(\"Randomized search\")\n",
    "print()\n",
    "    \n",
    "params = {'count__ngram_range': [(1, 1), (1, 2), (1, 3), (1, 4)],\n",
    "          'count__analyzer': [\"word\",\"char\"],\n",
    "          'classify__lr__dimensions': [100, 200]}\n",
    "\n",
    "grid_clf = RandomizedSearchCV(pipeline, params, n_jobs=-1, verbose=True, n_iter=8)\n",
    "grid_clf.fit(np.array(X_tr[:10000]), y_tr[:10000])\n",
    "\n",
    "best_params = sorted(grid_clf.grid_scores_, key=lambda x: -x[1])\n",
    "\n",
    "for params, score, _ in best_params:\n",
    "    print(score, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a>Double click to show the solution</a>\n",
    "<div class='spoiler'>\n",
    "\n",
    "from sklearn.grid_search import GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "print(\"Grid search\")\n",
    "print()\n",
    "\n",
    "params = {'vect__ngram_range': [(1, 1), (1, 2), (1, 3), (1, 4)],\n",
    "          'vect__analyzer': [\"word\",\"char\"],\n",
    "          'vect__binary': [True, False]}\n",
    "\n",
    "grid_clf = GridSearchCV(clf, params, n_jobs=1, verbose=True)\n",
    "grid_clf.fit(X_tr, y_tr)\n",
    "\n",
    "best_params = sorted(grid_clf.grid_scores_, key=lambda x: -x[1])\n",
    "\n",
    "\n",
    "for params, score, _ in best_params:\n",
    "    print(score, params) \n",
    "    \n",
    "print(\"Randomized search\")\n",
    "print()\n",
    "    \n",
    "params = {'vect__ngram_range': [(1, 1), (1, 2), (1, 3), (1, 4)],\n",
    "          'vect__analyzer': [\"word\",\"char\"],\n",
    "          'model__lr__dimensions': [100, 200]}\n",
    "\n",
    "grid_clf = RandomizedSearchCV(clf, params, n_jobs=1, verbose=True, n_iter=8)\n",
    "grid_clf.fit(np.array(X_tr[:10000]), y_tr[:10000])\n",
    "\n",
    "best_params = sorted(grid_clf.grid_scores_, key=lambda x: -x[1])\n",
    "\n",
    "for params, score, _ in best_params:\n",
    "    print(score, params)\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Useful materials\n",
    "\n",
    "1. http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html\n",
    "2. http://scikit-learn.org/stable/auto_examples/model_selection/randomized_search.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
