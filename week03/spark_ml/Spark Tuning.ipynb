{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.context.SparkContext at 0x7f00bee4fbd0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlc = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark ML Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elements:\n",
    "- Model (Estimator or Pipeline)\n",
    "- Set of ParamMaps (to perform the grid search) - you should use the ***ParamGridBuilder*** utility\n",
    "- Evaluator (to assess the fitness of the model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Splits the dataset into K folds\n",
    "- Each fold is splitted into a training (2/3) and a test (1/3) sets\n",
    "- It will fit K models and compute the average of the K evaluation metrics (according to the Evaluator)\n",
    "- Based on the metrics, it will determine the best set of parameters\n",
    "- Then it will fit the model one final time, using this set of parameters and the whole dataset\n",
    "- This is a VERY computationally expensive "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!rm -rf metastore_db/*.lck\n",
    "\n",
    "training = sqlc.createDataFrame([\n",
    "    (0, \"a b c d e spark\", 1.0),\n",
    "    (1, \"b d\", 0.0),\n",
    "    (2, \"spark f g h\", 1.0),\n",
    "    (3, \"hadoop mapreduce\", 0.0),\n",
    "    (4, \"b spark who\", 1.0),\n",
    "    (5, \"g d a y\", 0.0),\n",
    "    (6, \"spark fly\", 1.0),\n",
    "    (7, \"was mapreduce\", 0.0),\n",
    "    (8, \"e spark program\", 1.0),\n",
    "    (9, \"a e c l\", 0.0),\n",
    "    (10, \"spark compile\", 1.0),\n",
    "    (11, \"hadoop software\", 0.0)\n",
    "], [\"id\", \"text\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "\n",
    "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\n",
    "\n",
    "lr = LogisticRegression(maxIter=10)\n",
    "\n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(hashingTF.numFeatures, [10, 100, 1000]) \\\n",
    "    .addGrid(lr.regParam, [0.1, 0.01]) \\\n",
    "    .build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{Param(parent=u'HashingTF_4a57a3957b38db7942a4', name='numFeatures', doc='number of features.'): 10,\n",
       "  Param(parent=u'LogisticRegression_46a482905b563c13383e', name='regParam', doc='regularization parameter (>= 0).'): 0.1},\n",
       " {Param(parent=u'HashingTF_4a57a3957b38db7942a4', name='numFeatures', doc='number of features.'): 10,\n",
       "  Param(parent=u'LogisticRegression_46a482905b563c13383e', name='regParam', doc='regularization parameter (>= 0).'): 0.01},\n",
       " {Param(parent=u'HashingTF_4a57a3957b38db7942a4', name='numFeatures', doc='number of features.'): 100,\n",
       "  Param(parent=u'LogisticRegression_46a482905b563c13383e', name='regParam', doc='regularization parameter (>= 0).'): 0.1},\n",
       " {Param(parent=u'HashingTF_4a57a3957b38db7942a4', name='numFeatures', doc='number of features.'): 100,\n",
       "  Param(parent=u'LogisticRegression_46a482905b563c13383e', name='regParam', doc='regularization parameter (>= 0).'): 0.01},\n",
       " {Param(parent=u'HashingTF_4a57a3957b38db7942a4', name='numFeatures', doc='number of features.'): 1000,\n",
       "  Param(parent=u'LogisticRegression_46a482905b563c13383e', name='regParam', doc='regularization parameter (>= 0).'): 0.1},\n",
       " {Param(parent=u'HashingTF_4a57a3957b38db7942a4', name='numFeatures', doc='number of features.'): 1000,\n",
       "  Param(parent=u'LogisticRegression_46a482905b563c13383e', name='regParam', doc='regularization parameter (>= 0).'): 0.01}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paramGrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import CrossValidator\n",
    "\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=BinaryClassificationEvaluator(),\n",
    "                          numFolds=3)\n",
    "\n",
    "cvModel = crossval.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.75, 1.75, 1.75, 1.75, 1.8333333333333335, 1.8333333333333335]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvModel.avgMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PipelineModel_457d82a119e332275598"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvModel.bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Tokenizer_448c9b3d5cbe87b540d1,\n",
       " HashingTF_4a57a3957b38db7942a4,\n",
       " LogisticRegression_46a482905b563c13383e]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvModel.bestModel.stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lr_best = cvModel.bestModel.stages[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseVector(1000, {6: -1.4383, 66: -0.7317, 94: -0.4712, 105: 2.0311, 170: -0.1318, 181: -0.7851, 217: 0.9701, 234: -0.7317, 248: 0.755, 282: 0.755, 315: -0.7966, 361: 0.2486, 417: -0.0229, 463: 1.0866, 644: 0.8376, 695: 1.0866, 722: 0.2602, 878: 0.534, 953: -0.7851})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_best.coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lr_summary = lr_best.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_summary.areaUnderROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test = sqlc.createDataFrame([\n",
    "    (4L, \"spark i j k\"),\n",
    "    (5L, \"l m n\"),\n",
    "    (6L, \"mapreduce spark\"),\n",
    "    (7L, \"apache hadoop\")\n",
    "], [\"id\", \"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(id=4, text=u'spark i j k', probability=DenseVector([0.2661, 0.7339]), prediction=1.0)\n",
      "Row(id=5, text=u'l m n', probability=DenseVector([0.9209, 0.0791]), prediction=0.0)\n",
      "Row(id=6, text=u'mapreduce spark', probability=DenseVector([0.4429, 0.5571]), prediction=1.0)\n",
      "Row(id=7, text=u'apache hadoop', probability=DenseVector([0.8584, 0.1416]), prediction=0.0)\n"
     ]
    }
   ],
   "source": [
    "prediction = cvModel.transform(test)\n",
    "\n",
    "selected = prediction.select(\"id\", \"text\", \"probability\", \"prediction\")\n",
    "\n",
    "for row in selected.collect():\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Validation Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It uses the entire dataset\n",
    "- The dataset is splitted into a training and a test sets according to the ***trainRatio*** parameter\n",
    "- It will fit a model for each set of parameters and evaluate its metrics (according to the Evaluator)\n",
    "- Based on the metrics, it will determine the best set of parameters\n",
    "- Then it will fit the model one final time, using this set of parameters and the whole dataset\n",
    "- This is a much less expensive, but it may not yield good results if the dataset is not large enough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "data = sqlc.read.format(\"libsvm\").load(\"/home/ubuntu/spark/data/mllib/sample_linear_regression_data.txt\")\n",
    "\n",
    "train, test = data.randomSplit([0.7, 0.3])\n",
    "\n",
    "lr = LinearRegression(maxIter=20, regParam=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "\n",
    "paramGrid = ParamGridBuilder()\\\n",
    "    .addGrid(lr.regParam, [0.1, 0.01]) \\\n",
    "    .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])\\\n",
    "    .build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{Param(parent=u'LinearRegression_40c59fea604826a0431d', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 0.0,\n",
       "  Param(parent=u'LinearRegression_40c59fea604826a0431d', name='regParam', doc='regularization parameter (>= 0).'): 0.1},\n",
       " {Param(parent=u'LinearRegression_40c59fea604826a0431d', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 0.5,\n",
       "  Param(parent=u'LinearRegression_40c59fea604826a0431d', name='regParam', doc='regularization parameter (>= 0).'): 0.1},\n",
       " {Param(parent=u'LinearRegression_40c59fea604826a0431d', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 1.0,\n",
       "  Param(parent=u'LinearRegression_40c59fea604826a0431d', name='regParam', doc='regularization parameter (>= 0).'): 0.1},\n",
       " {Param(parent=u'LinearRegression_40c59fea604826a0431d', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 0.0,\n",
       "  Param(parent=u'LinearRegression_40c59fea604826a0431d', name='regParam', doc='regularization parameter (>= 0).'): 0.01},\n",
       " {Param(parent=u'LinearRegression_40c59fea604826a0431d', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 0.5,\n",
       "  Param(parent=u'LinearRegression_40c59fea604826a0431d', name='regParam', doc='regularization parameter (>= 0).'): 0.01},\n",
       " {Param(parent=u'LinearRegression_40c59fea604826a0431d', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 1.0,\n",
       "  Param(parent=u'LinearRegression_40c59fea604826a0431d', name='regParam', doc='regularization parameter (>= 0).'): 0.01}]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paramGrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import TrainValidationSplit\n",
    "\n",
    "tvs = TrainValidationSplit(estimator=lr,\n",
    "                           estimatorParamMaps=paramGrid,\n",
    "                           evaluator=RegressionEvaluator(),\n",
    "                           trainRatio=0.8)\n",
    "\n",
    "model = tvs.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression_40c59fea604826a0431d"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([-0.1492, 0.2969, -0.6552, 1.5598, 0.1965, 1.3713, 0.0, -0.2379, -0.7382, 0.4184])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.bestModel.coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.021874120623119664"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.bestModel.summary.r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prediction = model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>features</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-28.046018</td>\n",
       "      <td>(0.949330819585, 0.328521466154, 0.74930027801...</td>\n",
       "      <td>-1.569149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-23.487440</td>\n",
       "      <td>(-0.519535443126, 0.808035794841, 0.8498613208...</td>\n",
       "      <td>-0.796819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-20.057483</td>\n",
       "      <td>(-0.320505782811, 0.51605972927, 0.45215640988...</td>\n",
       "      <td>0.144287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-19.872991</td>\n",
       "      <td>(-0.932581077292, -0.641147114733, 0.994921629...</td>\n",
       "      <td>0.016299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-19.402336</td>\n",
       "      <td>(0.462288625222, -0.902975525943, 0.7442695642...</td>\n",
       "      <td>-0.602021</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       label                                           features  prediction\n",
       "0 -28.046018  (0.949330819585, 0.328521466154, 0.74930027801...   -1.569149\n",
       "1 -23.487440  (-0.519535443126, 0.808035794841, 0.8498613208...   -0.796819\n",
       "2 -20.057483  (-0.320505782811, 0.51605972927, 0.45215640988...    0.144287\n",
       "3 -19.872991  (-0.932581077292, -0.641147114733, 0.994921629...    0.016299\n",
       "4 -19.402336  (0.462288625222, -0.902975525943, 0.7442695642...   -0.602021"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction.toPandas()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
