{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.context.SparkContext at 0x7fb728405630>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext, Row\n",
    "sqlc = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2017-02-02 11:39:46--  https://raw.githubusercontent.com/6chaoran/DataStory/master/Titanic-Spark/train.csv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.112.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.112.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 61194 (60K) [text/plain]\n",
      "Saving to: ‘train.csv.4’\n",
      "\n",
      "train.csv.4         100%[===================>]  59.76K  --.-KB/s    in 0.09s   \n",
      "\n",
      "2017-02-02 11:39:46 (659 KB/s) - ‘train.csv.4’ saved [61194/61194]\n",
      "\n",
      "--2017-02-02 11:39:46--  https://raw.githubusercontent.com/6chaoran/DataStory/master/Titanic-Spark/test.csv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.112.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.112.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 28629 (28K) [text/plain]\n",
      "Saving to: ‘test.csv.4’\n",
      "\n",
      "test.csv.4          100%[===================>]  27.96K  --.-KB/s    in 0.03s   \n",
      "\n",
      "2017-02-02 11:39:46 (857 KB/s) - ‘test.csv.4’ saved [28629/28629]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/6chaoran/DataStory/master/Titanic-Spark/train.csv\n",
    "!wget https://raw.githubusercontent.com/6chaoran/DataStory/master/Titanic-Spark/test.csv\n",
    "\n",
    "data = sc.textFile('./train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PassengerId,Survived,Pclass,Name,Sex,Age,SibSp,Parch,Ticket,Fare,Cabin,Embarked',\n",
       " '1,0,3,\"Braund, Mr. Owen Harris\",male,22,1,0,A/5 21171,7.25,,S',\n",
       " '2,1,1,\"Cumings, Mrs. John Bradley (Florence Briggs Thayer)\",female,38,1,0,PC 17599,71.2833,C85,C',\n",
       " '3,1,3,\"Heikkinen, Miss. Laina\",female,26,0,0,STON/O2. 3101282,7.925,,S',\n",
       " '4,1,1,\"Futrelle, Mrs. Jacques Heath (Lily May Peel)\",female,35,1,0,113803,53.1,C123,S']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1\n",
    "- Split the fields and make sure to not include the header in the dataset itself.\n",
    "- Note some variables are categorical - use some transformation to encode them as numeric values\n",
    "- You may choose NOT to use all variables\n",
    "- Assembler all variables you chose to keep as a Dense Vector, so you finish this step with an RDD[Vector]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "891\n",
      "[Row(Age=22.0, Fare=7.25, Parch=0, Pclass=3, Sex=1, SibSp=1, Survived=0), Row(Age=38.0, Fare=71.2833, Parch=0, Pclass=1, Sex=0, SibSp=1, Survived=1), Row(Age=26.0, Fare=7.925, Parch=0, Pclass=3, Sex=0, SibSp=0, Survived=1), Row(Age=35.0, Fare=53.1, Parch=0, Pclass=1, Sex=0, SibSp=1, Survived=1), Row(Age=35.0, Fare=8.05, Parch=0, Pclass=3, Sex=1, SibSp=0, Survived=0), Row(Age=nan, Fare=8.4583, Parch=0, Pclass=3, Sex=1, SibSp=0, Survived=0)]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.linalg import Vectors, Vector\n",
    "from io import StringIO\n",
    "import numpy as np\n",
    "import csv\n",
    "import re\n",
    "\n",
    "## INSERT YOUR CODE HERE\n",
    "def load_csv(line):\n",
    "    return list(csv.reader(StringIO(line), delimiter=',', quotechar='\"'))[0]\n",
    "\n",
    "def readInt(x):\n",
    "    try:\n",
    "        return int(x)\n",
    "    except:\n",
    "        return np.NaN\n",
    "    \n",
    "def readFloat(x):\n",
    "    try:\n",
    "        return float(x)\n",
    "    except:\n",
    "        return np.NaN\n",
    "\n",
    "def readSex(x):\n",
    "    if x=='male':\n",
    "        return 1\n",
    "    else:\n",
    "        return 0 \n",
    "\n",
    "def parseElement(e):\n",
    "    return Row( Survived=readInt(e[1]),\n",
    "                Pclass=readInt(e[2]),\n",
    "                Sex=readSex(e[4]),\n",
    "                Age=readFloat(e[5]),\n",
    "                SibSp=readInt(e[6]),\n",
    "                Parch=readInt(e[7]),\n",
    "                Fare=readFloat(e[9])\n",
    "    )\n",
    "\n",
    "\n",
    "rdd = data.filter(lambda l: l.find(\"Age\") < 0)\n",
    "rdd = rdd.map(load_csv).map(parseElement).cache()\n",
    "\n",
    "\n",
    "print(rdd.count())\n",
    "\n",
    "print(rdd.take(6))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DenseVector([22.0, 7.25, 0.0, 3.0, 1.0, 1.0, 0.0]),\n",
       " DenseVector([38.0, 71.2833, 0.0, 1.0, 0.0, 1.0, 1.0]),\n",
       " DenseVector([26.0, 7.925, 0.0, 3.0, 0.0, 0.0, 1.0]),\n",
       " DenseVector([35.0, 53.1, 0.0, 1.0, 0.0, 1.0, 1.0]),\n",
       " DenseVector([35.0, 8.05, 0.0, 3.0, 1.0, 0.0, 0.0])]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic = rdd.map(Vectors.dense)\n",
    "type(titanic)\n",
    "titanic.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2\n",
    "- Explore the features you have assembled\n",
    "- You can use RDD operations as ***map*** and ***countByKey*** to check how many different values a given feature has and also its distribution\n",
    "- You can use ***Statistics*** to obtain statistics regarding your features - for instance, the corresponding means\n",
    "- Are there any ***NaN*** values in your dataset?\n",
    "- If so, define value/values to fill these ***NaN*** values\n",
    "    - hint: you can decompose the Dense vector using its ***values*** property and turning it into a list, change the desired value and reassemble it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {0.0: 608, 1.0: 209, 2.0: 28, 3.0: 16, 4.0: 18, 5.0: 5, 8.0: 7})\n",
      "[         nan  32.20420797   0.38159371   2.30864198   0.64758698\n",
      "   0.52300786   0.38383838]\n",
      "[ 891.  876.  213.  891.  577.  283.  342.]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.stat import Statistics\n",
    "\n",
    "## INSERT YOUR CODE HERE\n",
    "sibsp = titanic.map(lambda w: (w[5],1))\n",
    "print(sibsp.countByKey())\n",
    "\n",
    "summary = Statistics.colStats(titanic)\n",
    "print(summary.mean())\n",
    "print(summary.numNonzeros())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30.0\n"
     ]
    }
   ],
   "source": [
    "age = titanic.map(lambda w: w[0])\n",
    "ages = age.collect()\n",
    "mean_ages = np.round(np.nanmean(ages))\n",
    "print(mean_ages)\n",
    "age_no_nan = titanic.map(lambda w: w[0] if (not np.isnan(w[0])) else mean_ages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3\n",
    "- Make your RDD an RDD[LabeledPoint]\n",
    "- Train a classifier of your choice (for instance, Random Forest) using your dataset of LabeledPoints\n",
    "- Make predictions for the training data\n",
    "- Use the Binary Classification Metrics to evaluate your model on the training data\n",
    "- How is your model performing? Try to tune its parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LabeledPoint(0.0, [22.0,7.25,0.0,3.0,1.0,1.0]),\n",
       " LabeledPoint(1.0, [38.0,71.2833,0.0,1.0,0.0,1.0]),\n",
       " LabeledPoint(1.0, [26.0,7.925,0.0,3.0,0.0,0.0]),\n",
       " LabeledPoint(1.0, [35.0,53.1,0.0,1.0,0.0,1.0]),\n",
       " LabeledPoint(0.0, [35.0,8.05,0.0,3.0,1.0,0.0])]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.mllib.feature import LabeledPoint\n",
    "from pyspark.mllib.tree import RandomForest, RandomForestModel\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "\n",
    "### INSERT YOUR CODE HERE\n",
    "def make_labeled(row):\n",
    "    return LabeledPoint(row[-1],row[:-1])\n",
    "\n",
    "titanic_labeled = titanic.map(make_labeled)\n",
    "titanic_labeled.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = RandomForest.trainClassifier(titanic_labeled, \n",
    "                                     numClasses=2, \n",
    "                                     categoricalFeaturesInfo={},\n",
    "                                     numTrees=3, \n",
    "                                     featureSubsetStrategy=\"auto\",\n",
    "                                     impurity='gini', \n",
    "                                     maxDepth=4, \n",
    "                                     maxBins=32)\n",
    "\n",
    "#print(model.toDebugString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Error = 0.1717171717171717\n"
     ]
    }
   ],
   "source": [
    "labels = titanic_labeled.map(lambda x: x.label)\n",
    "features = titanic_labeled.map(lambda x: x.features)\n",
    "\n",
    "predictions = model.predict(features)\n",
    "\n",
    "labelsAndPredictions = labels.zip(predictions)\n",
    "\n",
    "testErr = labelsAndPredictions.filter(lambda args: args[0] != args[1]).count() / float(titanic_labeled.count())\n",
    "print('Train Error = ' + str(testErr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4\n",
    "- Apply the data processing/transforming steps to the testing data\n",
    "    - hint: it could be useful to revisit your past code and refactor it as function(s) taking an RDD as parameter and performing the operations on it\n",
    "- Make predictions for the test data\n",
    "- Save it as ***submission.csv*** and submit it to Kaggle\n",
    "- What was your score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PassengerId,Pclass,Name,Sex,Age,SibSp,Parch,Ticket,Fare,Cabin,Embarked', '892,3,\"Kelly, Mr. James\",male,34.5,0,0,330911,7.8292,,Q']\n",
      "[Row(Age=34.5, Fare=330911.0, Parch=0, Pclass=3, Sex=1, SibSp=0, Survived=0)]\n"
     ]
    }
   ],
   "source": [
    "### INSERT YOUR CODE HERE\n",
    "\n",
    "test = sc.textFile('./test.csv')\n",
    "\n",
    "def load_csv(line):\n",
    "    return list(csv.reader(StringIO(line), delimiter=',', quotechar='\"'))[0]\n",
    "\n",
    "def readInt(x):\n",
    "    try:\n",
    "        return int(x)\n",
    "    except:\n",
    "        return np.NaN\n",
    "    \n",
    "def readFloat(x):\n",
    "    try:\n",
    "        return float(x)\n",
    "    except:\n",
    "        return np.NaN\n",
    "\n",
    "def readSex(x):\n",
    "    if x=='male':\n",
    "        return 1\n",
    "    else:\n",
    "        return 0 \n",
    "\n",
    "def parseElement(e):\n",
    "    return Row( Survived=0,\n",
    "                Pclass=readInt(e[1]),\n",
    "                Sex=readSex(e[3]),\n",
    "                Age=readFloat(e[4]),\n",
    "                SibSp=readInt(e[5]),\n",
    "                Parch=readInt(e[6]),\n",
    "                Fare=readFloat(e[7])\n",
    "    )\n",
    "\n",
    "\n",
    "rdd2 = test.filter(lambda l: l.find(\"Age\") < 0)\n",
    "rdd2 = rdd2.map(load_csv).map(parseElement).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DenseVector([34.5, 330911.0, 0.0, 3.0, 1.0, 0.0, 0.0]),\n",
       " DenseVector([47.0, 363272.0, 0.0, 3.0, 0.0, 1.0, 0.0])]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic2 = rdd2.map(Vectors.dense)\n",
    "titanic2.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "titanic_labeled2 = titanic2.map(make_labeled)\n",
    "\n",
    "labels2 = titanic_labeled2.map(lambda x: x.label)\n",
    "features2 = titanic_labeled2.map(lambda x: x.features)\n",
    "\n",
    "predictions2 = model.predict(features2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(list(zip(range(892,1310), \n",
    "             predictions2.map(int).collect())), \n",
    "             columns=['PassengerId','Survived']).to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result = 77,033%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Getting the header\n",
    "header = data.first()\n",
    "# Removing the header\n",
    "data = data.filter(lambda row: row != header)\n",
    "\n",
    "# Parsing function - takes a row (string) and testing flag (boolean)\n",
    "# It already includes the option of processing a testing file\n",
    "def process_row(row, testing):\n",
    "    # If it is testing, \"Survived\" is not available in the file\n",
    "    # csv.reader is a very easy way to handle delimiters and quotes, but it only takes files - not string variables\n",
    "    # Comes StringIO to the rescue, since it allows you to treat a string variable as if it was a file\n",
    "    if testing:\n",
    "        passenger_id, pclass, name, sex, age, sibsp, parch, ticket, fare, cabin, embarked \\\n",
    "        = list(csv.reader(StringIO(row), delimiter=',', quotechar='\"'))[0]\n",
    "        survived = np.NaN\n",
    "    # If it is training, \"Survived\" is available\n",
    "    else:\n",
    "        passenger_id, survived, pclass, name, sex, age, sibsp, parch, ticket, fare, cabin, embarked \\\n",
    "        = list(csv.reader(StringIO(row), delimiter=',', quotechar='\"'))[0]\n",
    "        survived = int(survived)\n",
    "    # Encode the \"Sex\" variable as binary\n",
    "    sex = 1 if sex == 'male' else 0\n",
    "    # Encode the \"Embarked\" variable as 3 binary variables, one for each point of embark\n",
    "    cherbourg = 1 if embarked == 'C' else 0\n",
    "    queenstown = 1 if embarked == 'Q' else 0\n",
    "    southampton = 1 if embarked == 'S' else 0\n",
    "    # Try to cast age as float, making it NaN if it fails\n",
    "    try:\n",
    "        age = float(age)\n",
    "    except:\n",
    "        age = np.NaN\n",
    "    # Try to cast fare as float, making it NaN if it fails\n",
    "    try:\n",
    "        fare = float(fare)\n",
    "    except:\n",
    "        fare = np.NaN\n",
    "    # Assemble all variables as a dense vector\n",
    "    # Some variables are being casted at this moment - in this file, there were no NA values for these variables\n",
    "    # In a real-world problem, it could be the case we do not know ahead if there will be NA values or not\n",
    "    # So it is recommended to handle all of them as TRY...EXCEPT blocks\n",
    "    return Vectors.dense([survived, int(pclass), sex, age, int(sibsp), int(parch), fare, cherbourg, queenstown, southampton])\n",
    "\n",
    "# Assembling a list of the features, so it is easier to match the names with their statistics later\n",
    "features = ['Survived','PClass','sex','age','sibsp','parch','fare','cherbourg','queenstown','southampton']\n",
    "\n",
    "# Apply the parsing function to the training test (testing=False)\n",
    "rows = data.map(lambda row: process_row(row, False))\n",
    "\n",
    "# Defining a function to load and parse the data for a given filename and testing flag\n",
    "def load_data(filename, testing):\n",
    "    data = sc.textFile(filename)\n",
    "    header = data.first()\n",
    "    data = data.filter(lambda row: row != header)\n",
    "    rows = data.map(lambda row: process_row(row, testing))\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {0.0: 608, 1.0: 209, 2.0: 28, 3.0: 16, 4.0: 18, 5.0: 5, 8.0: 7})\n",
      "defaultdict(<class 'int'>, {1.0: 216, 2.0: 184, 3.0: 491})\n",
      "[('Survived', 342.0), ('PClass', 891.0), ('sex', 577.0), ('age', 891.0), ('sibsp', 283.0), ('parch', 213.0), ('fare', 876.0), ('cherbourg', 168.0), ('queenstown', 77.0), ('southampton', 644.0)]\n",
      "[('Survived', 0.38383838383838381), ('PClass', 2.308641975308642), ('sex', 0.6475869809203143), ('age', nan), ('sibsp', 0.52300785634118974), ('parch', 0.38159371492704819), ('fare', 32.204207968574657), ('cherbourg', 0.18855218855218855), ('queenstown', 0.086419753086419748), ('southampton', 0.72278338945005616)]\n",
      "29.6991176471\n",
      "[('Survived', 0.38383838383838381), ('PClass', 2.308641975308642), ('sex', 0.6475869809203143), ('age', 29.699117647058813), ('sibsp', 0.52300785634118974), ('parch', 0.38159371492704819), ('fare', 32.204207968574657), ('cherbourg', 0.18855218855218855), ('queenstown', 0.086419753086419748), ('southampton', 0.72278338945005616)]\n",
      "[('Survived', 1.0), ('PClass', -0.33848103596101498), ('sex', -0.54335138065775512), ('age', -0.069808515287141992), ('sibsp', -0.035322498885735604), ('parch', 0.081629407083483624), ('fare', 0.25730652238496216), ('cherbourg', 0.16824043121823321), ('queenstown', 0.0036503826839722003), ('southampton', -0.15566027340439345)]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.stat import Statistics\n",
    "\n",
    "# Exploring the SibSp variable - getting the count for each different value\n",
    "sibsp = rows.map(lambda row: (row[4], 1))\n",
    "print(sibsp.countByKey())\n",
    "\n",
    "# Exploring the PClass variable - getting the count for each different value\n",
    "pclass = rows.map(lambda row: (row[1], 1))\n",
    "print(pclass.countByKey())\n",
    "\n",
    "# Computing statistics over the RDD of rows (obtained at the end of Step 1)\n",
    "statistics = Statistics.colStats(rows)\n",
    "\n",
    "# The names of the features used in Step 1\n",
    "features = ['Survived','PClass','sex','age','sibsp','parch','fare','cherbourg','queenstown','southampton']\n",
    "\n",
    "# Checking the number of non-zeros for each feature\n",
    "print(list(zip(features, statistics.numNonzeros())))\n",
    "\n",
    "# Checking the mean for each feature\n",
    "# In this case, there is at leat one NaN value and therefore it affects the mean\n",
    "print(list(zip(features, statistics.mean())))\n",
    "\n",
    "# So, let's compute the mean of all numeric values\n",
    "# First, select only the \"Age\" feature\n",
    "age = rows.map(lambda row: row[3])\n",
    "# Then, filter out all NaN values\n",
    "age = age.filter(lambda age: not np.isnan(age))\n",
    "# And calculate the mean age\n",
    "ageMean = age.mean()\n",
    "print(ageMean)\n",
    "\n",
    "# Now it is time to replace all NaN values of age with its mean\n",
    "# So we slice the row.values, keeping the first 3, then doing the replacements at the 4th position\n",
    "# and appending the remaining values\n",
    "rowsWithAge = rows.map(lambda row: Vectors.dense(list(row.values[:3]) + [ageMean if np.isnan(row.values[3]) else row.values[3]] + list(row.values[4:])))\n",
    "\n",
    "# Defining a function to fill the missing age values using its mean\n",
    "def fill_age(rows):\n",
    "    age = rows.map(lambda row: row[3])\n",
    "    age = age.filter(lambda age: not np.isnan(age))\n",
    "    ageMean = age.mean()\n",
    "    rowsWithAge = rows.map(lambda row: Vectors.dense(list(row.values[:3]) + [ageMean if np.isnan(row.values[3]) else row.values[3]] + list(row.values[4:])))\n",
    "    return rowsWithAge\n",
    "\n",
    "# Computing statistics once again on the new RDD to make sure there are no NaN values anymore\n",
    "statistics2 = Statistics.colStats(rowsWithAge)\n",
    "\n",
    "# Now, the \"Age\" feature has a numeric mean\n",
    "print(list(zip(features, statistics2.mean())))\n",
    "\n",
    "# Compute the correlations between all variables - Survived is the first one, so we check its correlations\n",
    "print(list(zip(features, Statistics.corr(rowsWithAge)[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under ROC = 0.7438884095484614\n"
     ]
    }
   ],
   "source": [
    "# Make the row into a LabeledPoint - the label is the first element\n",
    "rowsLabeled = rowsWithAge.map(lambda row: LabeledPoint(row[0], Vectors.dense(row[1:])))\n",
    "\n",
    "# Train a simple and standard RandomForest classifier with 10 trees\n",
    "model = RandomForest.trainClassifier(rowsLabeled, \n",
    "                                     numClasses=2, \n",
    "                                     categoricalFeaturesInfo={},\n",
    "                                     numTrees=10, \n",
    "                                     featureSubsetStrategy=\"auto\",\n",
    "                                     impurity='gini', \n",
    "                                     maxDepth=4, \n",
    "                                     maxBins=32)\n",
    "\n",
    "# Split the labeled points into its labels and features\n",
    "labels = rowsLabeled.map(lambda x: x.label)\n",
    "features = rowsLabeled.map(lambda x: x.features)\n",
    "\n",
    "# Make predictions for the features using the RandomForest model\n",
    "# and then zip it together with its corresponding labels\n",
    "predictionAndLabels = model.predict(features).zip(labels)\n",
    "\n",
    "# Evaluate the metrics for the predictions made for the training set\n",
    "metrics = BinaryClassificationMetrics(predictionAndLabels)\n",
    "\n",
    "print(\"Area under ROC = %s\" % metrics.areaUnderROC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filename = 'test.csv'\n",
    "\n",
    "# Load the test data set, with testing=True\n",
    "rows = load_data(filename, True)\n",
    "\n",
    "# Fill the NA values for age\n",
    "rowsWithAge = fill_age(rows)\n",
    "\n",
    "# Make it an RDD of LabeledPoints\n",
    "rowsLabeled = rowsWithAge.map(lambda row: LabeledPoint(row[0], Vectors.dense(row[1:])))\n",
    "\n",
    "# Split the LabeledPoints into its labels and features\n",
    "labels = rowsLabeled.map(lambda x: x.label)\n",
    "features = rowsLabeled.map(lambda x: x.features)\n",
    "\n",
    "# Make predictions for the test set\n",
    "predictions = model.predict(features)\n",
    "\n",
    "# Generate a submission file for Kaggle\n",
    "pd.DataFrame(list(zip(range(892,1310), \n",
    "                 predictions.map(int).collect())), \n",
    "             columns=['PassengerId','Survived']).to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
