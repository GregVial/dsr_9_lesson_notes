{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.context.SparkContext at 0x7fa98c0a0bd0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!rm -rf metastore_db/*.lck\n",
    "\n",
    "from pyspark.sql import SQLContext\n",
    "sqlc = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1\n",
    "- Load the train and test sets\n",
    "- Check the schema, the variables have their right types?\n",
    "- If not, how to correctly load the datasets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = sqlc.read.csv(\"train.csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = sqlc.read.csv(\"test.csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+---+-----+-----+----------------+-------+-----+--------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex|Age|SibSp|Parch|          Ticket|   Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+--------------------+------+---+-----+-----+----------------+-------+-----+--------+\n",
      "|          1|       0|     3|Braund, Mr. Owen ...|  male| 22|    1|    0|       A/5 21171|   7.25|     |       S|\n",
      "|          2|       1|     1|Cumings, Mrs. Joh...|female| 38|    1|    0|        PC 17599|71.2833|  C85|       C|\n",
      "|          3|       1|     3|Heikkinen, Miss. ...|female| 26|    0|    0|STON/O2. 3101282|  7.925|     |       S|\n",
      "|          4|       1|     1|Futrelle, Mrs. Ja...|female| 35|    1|    0|          113803|   53.1| C123|       S|\n",
      "|          5|       0|     3|Allen, Mr. Willia...|  male| 35|    0|    0|          373450|   8.05|     |       S|\n",
      "|          6|       0|     3|    Moran, Mr. James|  male|   |    0|    0|          330877| 8.4583|     |       Q|\n",
      "|          7|       0|     1|McCarthy, Mr. Tim...|  male| 54|    0|    0|           17463|51.8625|  E46|       S|\n",
      "|          8|       0|     3|Palsson, Master. ...|  male|  2|    3|    1|          349909| 21.075|     |       S|\n",
      "|          9|       1|     3|Johnson, Mrs. Osc...|female| 27|    0|    2|          347742|11.1333|     |       S|\n",
      "|         10|       1|     2|Nasser, Mrs. Nich...|female| 14|    1|    0|          237736|30.0708|     |       C|\n",
      "|         11|       1|     3|Sandstrom, Miss. ...|female|  4|    1|    1|         PP 9549|   16.7|   G6|       S|\n",
      "|         12|       1|     1|Bonnell, Miss. El...|female| 58|    0|    0|          113783|  26.55| C103|       S|\n",
      "|         13|       0|     3|Saundercock, Mr. ...|  male| 20|    0|    0|       A/5. 2151|   8.05|     |       S|\n",
      "|         14|       0|     3|Andersson, Mr. An...|  male| 39|    1|    5|          347082| 31.275|     |       S|\n",
      "|         15|       0|     3|Vestrom, Miss. Hu...|female| 14|    0|    0|          350406| 7.8542|     |       S|\n",
      "|         16|       1|     2|Hewlett, Mrs. (Ma...|female| 55|    0|    0|          248706|     16|     |       S|\n",
      "|         17|       0|     3|Rice, Master. Eugene|  male|  2|    4|    1|          382652| 29.125|     |       Q|\n",
      "|         18|       1|     2|Williams, Mr. Cha...|  male|   |    0|    0|          244373|     13|     |       S|\n",
      "|         19|       0|     3|Vander Planke, Mr...|female| 31|    1|    0|          345763|     18|     |       S|\n",
      "|         20|       1|     3|Masselmani, Mrs. ...|female|   |    0|    0|            2649|  7.225|     |       C|\n",
      "+-----------+--------+------+--------------------+------+---+-----+-----+----------------+-------+-----+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- PassengerId: string (nullable = true)\n",
      " |-- Survived: string (nullable = true)\n",
      " |-- Pclass: string (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Age: string (nullable = true)\n",
      " |-- SibSp: string (nullable = true)\n",
      " |-- Parch: string (nullable = true)\n",
      " |-- Ticket: string (nullable = true)\n",
      " |-- Fare: string (nullable = true)\n",
      " |-- Cabin: string (nullable = true)\n",
      " |-- Embarked: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "177"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.createOrReplaceTempView(\"passengers\")\n",
    "sqlc.sql(\"select * from passengers where age =''\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "\n",
    "customSchema = StructType([StructField(\"PassengerId\", IntegerType(), True),\n",
    "                           StructField(\"Survived\", DoubleType(), True),\n",
    "                           StructField(\"Pclass\", IntegerType(), True), \n",
    "                           StructField(\"Name\", StringType(), True),\n",
    "                           StructField(\"Sex\", StringType(), True),\n",
    "                           StructField(\"Age\", DoubleType(), True),\n",
    "                           StructField(\"SibSp\", IntegerType(), True),\n",
    "                           StructField(\"Parch\", IntegerType(), True),\n",
    "                           StructField(\"Ticket\", StringType(), True),\n",
    "                           StructField(\"Fare\", DoubleType(), True),\n",
    "                           StructField(\"Cabin\", StringType(), True),\n",
    "                           StructField(\"Embarked\", StringType(), True)])\n",
    "\n",
    "customSchema2 = StructType([StructField(\"PassengerId\", IntegerType(), True),\n",
    "                           StructField(\"Pclass\", IntegerType(), True), \n",
    "                           StructField(\"Name\", StringType(), True),\n",
    "                           StructField(\"Sex\", StringType(), True),\n",
    "                           StructField(\"Age\", DoubleType(), True),\n",
    "                           StructField(\"SibSp\", IntegerType(), True),\n",
    "                           StructField(\"Parch\", IntegerType(), True),\n",
    "                           StructField(\"Ticket\", StringType(), True),\n",
    "                           StructField(\"Fare\", DoubleType(), True),\n",
    "                           StructField(\"Cabin\", StringType(), True),\n",
    "                           StructField(\"Embarked\", StringType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = sqlc.read.csv(\"train.csv\", header=True, schema=customSchema)\n",
    "test = sqlc.read.csv(\"test.csv\", header=True, schema=customSchema2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- PassengerId: integer (nullable = true)\n",
      " |-- Survived: double (nullable = true)\n",
      " |-- Pclass: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- SibSp: integer (nullable = true)\n",
      " |-- Parch: integer (nullable = true)\n",
      " |-- Ticket: string (nullable = true)\n",
      " |-- Fare: double (nullable = true)\n",
      " |-- Cabin: string (nullable = true)\n",
      " |-- Embarked: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- PassengerId: integer (nullable = true)\n",
      " |-- Pclass: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- SibSp: integer (nullable = true)\n",
      " |-- Parch: integer (nullable = true)\n",
      " |-- Ticket: string (nullable = true)\n",
      " |-- Fare: double (nullable = true)\n",
      " |-- Cabin: string (nullable = true)\n",
      " |-- Embarked: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2\n",
    "- Explore the features of your dataset\n",
    "- You can use DataFrame's ***describe*** method to get summary statistics\n",
    "    - hint: ***toPandas*** may be useful to ease the manipulation of small dataframes\n",
    "- Are there any ***NaN*** values in your dataset?\n",
    "- If so, define value/values to fill these ***NaN*** values\n",
    "    - hint: ***na*** property of DataFrames provide several methods of handling NA values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>summary</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>891</td>\n",
       "      <td>891</td>\n",
       "      <td>891</td>\n",
       "      <td>714</td>\n",
       "      <td>891</td>\n",
       "      <td>891</td>\n",
       "      <td>891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>446.0</td>\n",
       "      <td>0.3838383838383838</td>\n",
       "      <td>2.308641975308642</td>\n",
       "      <td>29.69911764705882</td>\n",
       "      <td>0.5230078563411896</td>\n",
       "      <td>0.38159371492704824</td>\n",
       "      <td>32.2042079685746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stddev</th>\n",
       "      <td>257.3538420152301</td>\n",
       "      <td>0.48659245426485753</td>\n",
       "      <td>0.8360712409770491</td>\n",
       "      <td>14.526497332334035</td>\n",
       "      <td>1.1027434322934315</td>\n",
       "      <td>0.8060572211299488</td>\n",
       "      <td>49.69342859718089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>891</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>80.0</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>512.3292</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               PassengerId             Survived              Pclass  \\\n",
       "summary                                                               \n",
       "count                  891                  891                 891   \n",
       "mean                 446.0   0.3838383838383838   2.308641975308642   \n",
       "stddev   257.3538420152301  0.48659245426485753  0.8360712409770491   \n",
       "min                      1                  0.0                   1   \n",
       "max                    891                  1.0                   3   \n",
       "\n",
       "                        Age               SibSp                Parch  \\\n",
       "summary                                                                \n",
       "count                   714                 891                  891   \n",
       "mean      29.69911764705882  0.5230078563411896  0.38159371492704824   \n",
       "stddev   14.526497332334035  1.1027434322934315   0.8060572211299488   \n",
       "min                    0.42                   0                    0   \n",
       "max                    80.0                   8                    6   \n",
       "\n",
       "                      Fare  \n",
       "summary                     \n",
       "count                  891  \n",
       "mean      32.2042079685746  \n",
       "stddev   49.69342859718089  \n",
       "min                    0.0  \n",
       "max               512.3292  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_desc = train.describe().toPandas().set_index('summary')\n",
    "train_desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Fare': 0.2573065223849626, 'Age': 0.010539215871285682, 'SibSp': -0.0353224988857356, 'Pclass': -0.3384810359610151, 'Parch': 0.08162940708348339}\n"
     ]
    }
   ],
   "source": [
    "print {col:train.stat.corr('Survived',col) for col in ['Pclass','Age','SibSp','Parch','Fare']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Fare': 0, 'Name': 0, 'Embarked': 0, 'Age': 177, 'Parch': 0, 'Pclass': 0, 'Sex': 0, 'Survived': 0, 'SibSp': 0, 'PassengerId': 0, 'Ticket': 0, 'Cabin': 0}\n"
     ]
    }
   ],
   "source": [
    "print {col:train.where(train[col].isNull()).count() for col in train.columns}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ageMean = float(train_desc.loc['mean']['Age'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainFilled = train.na.fill({'Age': ageMean})\n",
    "testFilled = test.na.fill({'Age': ageMean})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3\n",
    "- How to handle categorical features?\n",
    "    - hint: check the Estimators and Transformers\n",
    "- Assemble all desired features into a Vector using the VectorAssembler Transformer\n",
    "- Make sure to end up with a DataFrame with two columns: ***Survived*** and ***vFeatures***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "indexer = StringIndexer().setInputCol(\"Embarked\").setOutputCol(\"nEmbarked\")\n",
    "indexed1 = indexer.fit(trainFilled).transform(trainFilled)\n",
    "\n",
    "indexer = StringIndexer().setInputCol(\"Sex\").setOutputCol(\"nSex\")\n",
    "indexed2 = indexer.fit(indexed1).transform(indexed1)\n",
    "\n",
    "encoder = OneHotEncoder().setInputCol(\"nEmbarked\").setOutputCol(\"vEmbarked\")\n",
    "encoded1 = encoder.transform(indexed2)\n",
    "\n",
    "encoder = OneHotEncoder().setInputCol(\"nSex\").setOutputCol(\"vSex\")\n",
    "encoded2 = encoder.transform(encoded1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+---------+----+-------------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|          Ticket|   Fare|Cabin|Embarked|nEmbarked|nSex|    vEmbarked|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+---------+----+-------------+\n",
      "|          1|     0.0|     3|Braund, Mr. Owen ...|  male|22.0|    1|    0|       A/5 21171|   7.25|     |       S|      0.0| 0.0|(3,[0],[1.0])|\n",
      "|          2|     1.0|     1|Cumings, Mrs. Joh...|female|38.0|    1|    0|        PC 17599|71.2833|  C85|       C|      1.0| 1.0|(3,[1],[1.0])|\n",
      "|          3|     1.0|     3|Heikkinen, Miss. ...|female|26.0|    0|    0|STON/O2. 3101282|  7.925|     |       S|      0.0| 1.0|(3,[0],[1.0])|\n",
      "|          4|     1.0|     1|Futrelle, Mrs. Ja...|female|35.0|    1|    0|          113803|   53.1| C123|       S|      0.0| 1.0|(3,[0],[1.0])|\n",
      "|          5|     0.0|     3|Allen, Mr. Willia...|  male|35.0|    0|    0|          373450|   8.05|     |       S|      0.0| 0.0|(3,[0],[1.0])|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+---------+----+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "encoded1.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.mllib.util import MLUtils\n",
    "\n",
    "assembler = VectorAssembler(inputCols=['Pclass','Age','SibSp','Parch','Fare','vSex','vEmbarked'], outputCol='vFeatures')\n",
    "assembled = assembler.transform(encoded2)\n",
    "\n",
    "assembled2 = assembled.select(\"Survived\",\"vFeatures\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4\n",
    "- In Step 5, you will apply a normalization Estimator\n",
    "- BUT, it does not accept feature vectors of the Sparse type\n",
    "- So, it is neccessary to apply an User Defined Function to make all features vectors of type VectorUDT\n",
    "- In this step, you only have to replace ***YOUR DATAFRAME*** and ***NEW DATAFRAME*** with your variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import UserDefinedFunction\n",
    "from pyspark.ml.linalg import VectorUDT, Vectors\n",
    "\n",
    "to_vec = UserDefinedFunction(lambda x: Vectors.dense(x.toArray()), VectorUDT())\n",
    "\n",
    "assembled3 = assembled2.select(\"Survived\", to_vec(\"vFeatures\").alias(\"features\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5\n",
    "- Apply a normalization Estimator of your choice to the ***features*** vector obtained in Step 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "scaler = StandardScaler().setInputCol(\"features\").setOutputCol(\"scaledFeat\").setWithStd(True).setWithMean(True)\n",
    "scalerModel = scaler.fit(assembled3)\n",
    "scaled = scalerModel.transform(assembled3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6\n",
    "- Train a classifier of your choice (for instance, Random Forest) using your dataset of LabeledPoints\n",
    "- Make predictions for the training data\n",
    "- Use the Binary Classification Evaluator to evaluate your model on the training data\n",
    "- How is your model performing? Try to tune its parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.classification import RandomForestClassificationModel\n",
    "\n",
    "rfC = RandomForestClassifier().setLabelCol(\"Survived\") \\\n",
    "                                .setFeaturesCol(\"scaledFeat\") \\\n",
    "                                .setNumTrees(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = rfC.fit(scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|Survived|            features|          scaledFeat|       rawPrediction|         probability|prediction|\n",
      "+--------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|     0.0|[3.0,22.0,1.0,0.0...|[0.82691281652436...|[45.1444549359466...|[0.90288909871893...|       0.0|\n",
      "|     1.0|[1.0,38.0,1.0,0.0...|[-1.5652278312782...|[2.07990312533043...|[0.04159806250660...|       1.0|\n",
      "|     1.0|[3.0,26.0,0.0,0.0...|[0.82691281652436...|[26.7242611681227...|[0.53448522336245...|       0.0|\n",
      "|     1.0|[1.0,35.0,1.0,0.0...|[-1.5652278312782...|[5.35564640559562...|[0.10711292811191...|       1.0|\n",
      "|     0.0|[3.0,35.0,0.0,0.0...|[0.82691281652436...|[44.1763574665143...|[0.88352714933028...|       0.0|\n",
      "|     0.0|[3.0,29.699117647...|[0.82691281652436...|[43.6161033303125...|[0.87232206660625...|       0.0|\n",
      "|     0.0|[1.0,54.0,0.0,0.0...|[-1.5652278312782...|[34.0854046262521...|[0.68170809252504...|       0.0|\n",
      "|     0.0|[3.0,2.0,3.0,1.0,...|[0.82691281652436...|[36.0271530284997...|[0.72054306056999...|       0.0|\n",
      "|     1.0|[3.0,27.0,0.0,2.0...|[0.82691281652436...|[22.7261312637047...|[0.45452262527409...|       1.0|\n",
      "|     1.0|[2.0,14.0,1.0,0.0...|[-0.3691575073769...|[6.55453831921173...|[0.13109076638423...|       1.0|\n",
      "|     1.0|[3.0,4.0,1.0,1.0,...|[0.82691281652436...|[19.6900862767525...|[0.39380172553505...|       1.0|\n",
      "|     1.0|[1.0,58.0,0.0,0.0...|[-1.5652278312782...|[9.20949252994948...|[0.18418985059898...|       1.0|\n",
      "|     0.0|[3.0,20.0,0.0,0.0...|[0.82691281652436...|[44.1088376850440...|[0.88217675370088...|       0.0|\n",
      "|     0.0|[3.0,39.0,1.0,5.0...|[0.82691281652436...|[41.9437941215622...|[0.83887588243124...|       0.0|\n",
      "|     0.0|[3.0,14.0,0.0,0.0...|[0.82691281652436...|[25.5333193936293...|[0.51066638787258...|       0.0|\n",
      "|     1.0|[2.0,55.0,0.0,0.0...|[-0.3691575073769...|[10.1297492679873...|[0.20259498535974...|       1.0|\n",
      "|     0.0|[3.0,2.0,4.0,1.0,...|[0.82691281652436...|[39.4925933759222...|[0.78985186751844...|       0.0|\n",
      "|     1.0|[2.0,29.699117647...|[-0.3691575073769...|[42.2881530314557...|[0.84576306062911...|       0.0|\n",
      "|     0.0|[3.0,31.0,1.0,0.0...|[0.82691281652436...|[25.4043023075825...|[0.50808604615165...|       0.0|\n",
      "|     1.0|[3.0,29.699117647...|[0.82691281652436...|[17.6271181586143...|[0.35254236317228...|       1.0|\n",
      "+--------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = model.transform(scaled)\n",
    "predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.12343815,  0.07198568,  0.03726658,  0.04504931,  0.1717449 ,\n",
       "        0.50233138,  0.0200575 ,  0.01623386,  0.01189264])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.featureImportances.toArray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator().setLabelCol(\"Survived\") \\\n",
    "                            .setRawPredictionCol(\"rawPrediction\") \\\n",
    "                            .setMetricName(\"areaUnderROC\")\n",
    "\n",
    "roc = evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.904864240138903"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Step 7\n",
    "- Take a look at the test data - use DataFrame's ***createOrReplaceTempView*** method to perform SQL queries over the data\n",
    "    - hint: check if there are any NULL values in the dataset - if so, handle them\n",
    "- Apply the transformations to the test data\n",
    "    - hint: you can use Pipelines to chain several Estimators/Transformers\n",
    "    - warning: unfortunately, it is not possible to include the UDF from Step 4 in the Pipeline\n",
    "- Make predictions using the model previously trained and the transformed test data\n",
    "- Save it as ***submission.csv*** and submit it to Kaggle\n",
    "- What was your score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "testFilled.createOrReplaceTempView('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Age': 0,\n",
       " 'Cabin': 0,\n",
       " 'Embarked': 0,\n",
       " 'Fare': 1,\n",
       " 'Name': 0,\n",
       " 'Parch': 0,\n",
       " 'PassengerId': 0,\n",
       " 'Pclass': 0,\n",
       " 'Sex': 0,\n",
       " 'SibSp': 0,\n",
       " 'Ticket': 0}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{col: sqlc.sql(\"select * from test where \" + col + \" is null\").count() for col in testFilled.columns}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1044</td>\n",
       "      <td>3</td>\n",
       "      <td>Storey, Mr. Thomas</td>\n",
       "      <td>male</td>\n",
       "      <td>60.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3701</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Pclass                Name   Sex   Age  SibSp  Parch Ticket  \\\n",
       "0         1044       3  Storey, Mr. Thomas  male  60.5      0      0   3701   \n",
       "\n",
       "   Fare Cabin Embarked  \n",
       "0  None              S  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlc.sql(\"select * from test where Fare is null\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.6755501018\n"
     ]
    }
   ],
   "source": [
    "trainFilled.createOrReplaceTempView('train')\n",
    "avgFare = sqlc.sql(\"select mean(Fare) from train where Pclass = 3\").take(1)[0][0]\n",
    "print avgFare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "testFilled = testFilled.na.fill({'Fare': avgFare})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "def process_features(train, test):\n",
    "    indexer1 = StringIndexer().setInputCol(\"Embarked\").setOutputCol(\"nEmbarked\")\n",
    "    indexer2 = StringIndexer().setInputCol(\"Sex\").setOutputCol(\"nSex\")\n",
    "    encoder1 = OneHotEncoder().setInputCol(\"nEmbarked\").setOutputCol(\"vEmbarked\")\n",
    "    encoder2 = OneHotEncoder().setInputCol(\"nSex\").setOutputCol(\"vSex\")\n",
    "    assembler = VectorAssembler(inputCols=['Pclass','Age','SibSp','Parch','Fare','vSex','vEmbarked'], outputCol='vFeatures')\n",
    "\n",
    "    pipeFeat = Pipeline().setStages([indexer1, indexer2, encoder1, encoder2, assembler])\n",
    "    \n",
    "    model = pipeFeat.fit(train)\n",
    "    trainFeat = model.transform(train).select(\"Survived\", to_vec(\"vFeatures\").alias(\"features\"))   \n",
    "    scaler = StandardScaler().setInputCol(\"features\").setOutputCol(\"scaledFeat\").setWithStd(True).setWithMean(True).fit(trainFeat)\n",
    "    \n",
    "    testFeat = model.transform(test).select(to_vec(\"vFeatures\").alias(\"features\"))\n",
    "    \n",
    "    return scaler.transform(trainFeat), scaler.transform(testFeat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainFeat, testFeat = process_features(trainFilled, testFilled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rfC = RandomForestClassifier().setLabelCol(\"Survived\") \\\n",
    "                                .setFeaturesCol(\"scaledFeat\") \\\n",
    "                                .setNumTrees(50)\n",
    "model = rfC.fit(trainFeat)\n",
    "\n",
    "predictions = model.transform(testFeat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|            features|          scaledFeat|       rawPrediction|         probability|prediction|\n",
      "+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|[3.0,34.5,0.0,0.0...|[0.82691281652436...|[44.7826553865377...|[0.89565310773075...|       0.0|\n",
      "|[3.0,47.0,1.0,0.0...|[0.82691281652436...|[29.3444347833123...|[0.58688869566624...|       0.0|\n",
      "|[2.0,62.0,0.0,0.0...|[-0.3691575073769...|[43.4280104170114...|[0.86856020834022...|       0.0|\n",
      "|[3.0,27.0,0.0,0.0...|[0.82691281652436...|[43.9161577225803...|[0.87832315445160...|       0.0|\n",
      "|[3.0,22.0,1.0,1.0...|[0.82691281652436...|[21.5720901613269...|[0.43144180322653...|       1.0|\n",
      "|[3.0,14.0,0.0,0.0...|[0.82691281652436...|[36.1721767632747...|[0.72344353526549...|       0.0|\n",
      "|[3.0,30.0,0.0,0.0...|[0.82691281652436...|[20.4702808904097...|[0.40940561780819...|       1.0|\n",
      "|[2.0,26.0,1.0,1.0...|[-0.3691575073769...|[34.236073477771,...|[0.68472146955542...|       0.0|\n",
      "|[3.0,18.0,0.0,0.0...|[0.82691281652436...|[16.2468692408654...|[0.32493738481730...|       1.0|\n",
      "|[3.0,21.0,2.0,0.0...|[0.82691281652436...|[42.8232016051502...|[0.85646403210300...|       0.0|\n",
      "|[3.0,29.699117647...|[0.82691281652436...|[44.7728832291757...|[0.89545766458351...|       0.0|\n",
      "|[1.0,46.0,0.0,0.0...|[-1.5652278312782...|[39.5563196337216...|[0.79112639267443...|       0.0|\n",
      "|[1.0,23.0,1.0,0.0...|[-1.5652278312782...|[2.85335227504182...|[0.05706704550083...|       1.0|\n",
      "|[2.0,63.0,1.0,0.0...|[-0.3691575073769...|[38.5087610102198...|[0.77017522020439...|       0.0|\n",
      "|[1.0,47.0,1.0,0.0...|[-1.5652278312782...|[4.81273748831092...|[0.09625474976621...|       1.0|\n",
      "|[2.0,24.0,1.0,0.0...|[-0.3691575073769...|[6.61540903894548...|[0.13230818077890...|       1.0|\n",
      "|[2.0,35.0,0.0,0.0...|[-0.3691575073769...|[42.4471282176119...|[0.84894256435223...|       0.0|\n",
      "|[3.0,21.0,0.0,0.0...|[0.82691281652436...|[40.8092851027632...|[0.81618570205526...|       0.0|\n",
      "|[3.0,27.0,1.0,0.0...|[0.82691281652436...|[27.7669097607130...|[0.55533819521426...|       0.0|\n",
      "|[3.0,45.0,0.0,0.0...|[0.82691281652436...|[23.6457803401109...|[0.47291560680221...|       1.0|\n",
      "+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_predictions = predictions.select(\"prediction\").toPandas().reset_index()\n",
    "df_predictions['index'] = df_predictions['index'] + 892\n",
    "df_predictions.columns = ['PassengerId', 'Survived']\n",
    "\n",
    "df_predictions.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result = 75.598%"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
