{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# needed whenever working with spark dataframes\n",
    "from pyspark.sql import *\n",
    "!rm -rf metastore_db/\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2017-01-25 10:23:10--  https://ibm.box.com/shared/static/f1dhhjnzjwxmy2c1ys2whvrgz05d1pui.csv\n",
      "Resolving ibm.box.com (ibm.box.com)... 107.152.27.197, 107.152.26.197\n",
      "Connecting to ibm.box.com (ibm.box.com)|107.152.27.197|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://ibm.ent.box.com/shared/static/f1dhhjnzjwxmy2c1ys2whvrgz05d1pui.csv [following]\n",
      "--2017-01-25 10:23:11--  https://ibm.ent.box.com/shared/static/f1dhhjnzjwxmy2c1ys2whvrgz05d1pui.csv\n",
      "Resolving ibm.ent.box.com (ibm.ent.box.com)... 74.112.184.69, 74.112.185.69, 107.152.24.211, ...\n",
      "Connecting to ibm.ent.box.com (ibm.ent.box.com)|74.112.184.69|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://public.boxcloud.com/d/1/now8-1MPDuVw7f7t7c_x7aaLbXGESCq5aPrxe38cmrD4c0Vh10badFA_dx67XhRBuIVueKLLowGskMM-tje3XBY9Nzw3agxiW3UJ5IDtttPV_GnYR4Jno9pJ6SRpjPG0ILMTXRI6qnxZcRjzmLEyqZi6jpMVp5zwOV_4LeS2qy4ZG1sPzUSUPp0HL-KCxCwZJKscBp63cCjzpsx9PYDfWEe0soxOr_43-814NThlCa0Pzx2gAKEPgF7PnvBBEC-O_hQSXHN2nuT0mTxw7gI66iSuFgjh3nDodvW6T1acKDTgTXD39XbIHN3Dx_k8xaZ59bV1iUi8Hf--MbO63l_F0HstMh-VY7IPZ1rDSPJrTHYKTyw3hA30-eYIwtFLmCrtzTxc-cj-zbDxYyihjeEzTpPZySI3SKKtCx45aayuBjCsaRYBVwiKfcGZzvI6wANPKZy1G2sN5fMKrvkxeGoazbFqqz0RSeE6GC74cjfwNgYOzJPc5PZM1ZRMeK5NL5XVSoiMRp_ESf8srC5Yr21zJt_nnAFwpEyKbmP8FVyMWjGSE2pNX14mpvMzjO_pbQtdnB9npAbGoae6jl9ij7gRpeOWFD2hRUb48RTAGIknJ4b3hgmuwquK8eh5mezsHaW9uhw-jXCdEDH_BfMRlzKVtGVnkk12bI3evSJGZz_j7Xf4-m1c29ITfEOYzg1KK8I_NAo2XMnUnJxmpCoEU7i-QBazyLtF5-dBIqpizYC-dCrhcmqMVUyHBNxj2schtn67UoQ6lKV9vVe25vQ7EWwHv-zlBe-mKbOBV4FbpETExEiYS8o67H9rTRL4Kli7KUf2SxX26jVTuUO2coMau4AJnI0wGLZ1cpooSHGoFpfA9gnXOUlapo2E6A_ZUhC5Ttv24A7dQsUxo5Xh-S8MSu44CmJ1RCO6uUaO3y7w_U2wzu6PJEaBm2aVR1Fz7mZflewZG0uIwSMRBQcH4yyzpzayIY4wwe4oNgw_Ob_lWl4WTAFrwj8D8iBGQMBkpILGlWdJiEbla7kwWaQgvR0c67n9Ey0Wg-9yKfPZCIfk1C_iHJgSEI6iIlQXSVWo-PmnWtkaPZVTjXYo-NdTHNdXCuNq6IpdJ-WISuQ5vF1uC6kaVqIlyE7fkLedCwGsz_7RNDhkbfTk3JXnzLW3MCSS0qy6tjMQqAklV0k5mdKVR_1qISCLcg_l8E9suNKblw../download [following]\n",
      "--2017-01-25 10:23:13--  https://public.boxcloud.com/d/1/now8-1MPDuVw7f7t7c_x7aaLbXGESCq5aPrxe38cmrD4c0Vh10badFA_dx67XhRBuIVueKLLowGskMM-tje3XBY9Nzw3agxiW3UJ5IDtttPV_GnYR4Jno9pJ6SRpjPG0ILMTXRI6qnxZcRjzmLEyqZi6jpMVp5zwOV_4LeS2qy4ZG1sPzUSUPp0HL-KCxCwZJKscBp63cCjzpsx9PYDfWEe0soxOr_43-814NThlCa0Pzx2gAKEPgF7PnvBBEC-O_hQSXHN2nuT0mTxw7gI66iSuFgjh3nDodvW6T1acKDTgTXD39XbIHN3Dx_k8xaZ59bV1iUi8Hf--MbO63l_F0HstMh-VY7IPZ1rDSPJrTHYKTyw3hA30-eYIwtFLmCrtzTxc-cj-zbDxYyihjeEzTpPZySI3SKKtCx45aayuBjCsaRYBVwiKfcGZzvI6wANPKZy1G2sN5fMKrvkxeGoazbFqqz0RSeE6GC74cjfwNgYOzJPc5PZM1ZRMeK5NL5XVSoiMRp_ESf8srC5Yr21zJt_nnAFwpEyKbmP8FVyMWjGSE2pNX14mpvMzjO_pbQtdnB9npAbGoae6jl9ij7gRpeOWFD2hRUb48RTAGIknJ4b3hgmuwquK8eh5mezsHaW9uhw-jXCdEDH_BfMRlzKVtGVnkk12bI3evSJGZz_j7Xf4-m1c29ITfEOYzg1KK8I_NAo2XMnUnJxmpCoEU7i-QBazyLtF5-dBIqpizYC-dCrhcmqMVUyHBNxj2schtn67UoQ6lKV9vVe25vQ7EWwHv-zlBe-mKbOBV4FbpETExEiYS8o67H9rTRL4Kli7KUf2SxX26jVTuUO2coMau4AJnI0wGLZ1cpooSHGoFpfA9gnXOUlapo2E6A_ZUhC5Ttv24A7dQsUxo5Xh-S8MSu44CmJ1RCO6uUaO3y7w_U2wzu6PJEaBm2aVR1Fz7mZflewZG0uIwSMRBQcH4yyzpzayIY4wwe4oNgw_Ob_lWl4WTAFrwj8D8iBGQMBkpILGlWdJiEbla7kwWaQgvR0c67n9Ey0Wg-9yKfPZCIfk1C_iHJgSEI6iIlQXSVWo-PmnWtkaPZVTjXYo-NdTHNdXCuNq6IpdJ-WISuQ5vF1uC6kaVqIlyE7fkLedCwGsz_7RNDhkbfTk3JXnzLW3MCSS0qy6tjMQqAklV0k5mdKVR_1qISCLcg_l8E9suNKblw../download\n",
      "Resolving public.boxcloud.com (public.boxcloud.com)... 107.152.24.200, 74.112.185.96, 107.152.25.200, ...\n",
      "Connecting to public.boxcloud.com (public.boxcloud.com)|107.152.24.200|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1697 (1.7K) [text/csv]\n",
      "Saving to: ‘f1dhhjnzjwxmy2c1ys2whvrgz05d1pui.csv’\n",
      "\n",
      "f1dhhjnzjwxmy2c1ys2 100%[===================>]   1.66K  --.-KB/s    in 0s      \n",
      "\n",
      "2017-01-25 10:23:14 (31.1 MB/s) - ‘f1dhhjnzjwxmy2c1ys2whvrgz05d1pui.csv’ saved [1697/1697]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://ibm.box.com/shared/static/f1dhhjnzjwxmy2c1ys2whvrgz05d1pui.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# using inferschema is very handy, but adds a second pass over the data\n",
    "\n",
    "df = sqlContext.read.format('com.databricks.spark.csv')\\\n",
    "                .option('header', 'true')\\\n",
    "                .option('inferschema', 'true')\\\n",
    "                .option('mode', 'DROPMALFORMED')\\\n",
    "                .load('f1dhhjnzjwxmy2c1ys2whvrgz05d1pui.csv')\n",
    "#look at Spark Web UI to see the two passes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- car: string (nullable = true)\n",
      " |-- mpg: double (nullable = true)\n",
      " |-- cyl: integer (nullable = true)\n",
      " |-- disp: double (nullable = true)\n",
      " |-- hp: integer (nullable = true)\n",
      " |-- drat: double (nullable = true)\n",
      " |-- wt: double (nullable = true)\n",
      " |-- qsec: double (nullable = true)\n",
      " |-- vs: integer (nullable = true)\n",
      " |-- am: integer (nullable = true)\n",
      " |-- gear: integer (nullable = true)\n",
      " |-- carb: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----+---+-----+---+----+-----+-----+---+---+----+----+\n",
      "|              car| mpg|cyl| disp| hp|drat|   wt| qsec| vs| am|gear|carb|\n",
      "+-----------------+----+---+-----+---+----+-----+-----+---+---+----+----+\n",
      "|        Mazda RX4|21.0|  6|160.0|110| 3.9| 2.62|16.46|  0|  1|   4|   4|\n",
      "|    Mazda RX4 Wag|21.0|  6|160.0|110| 3.9|2.875|17.02|  0|  1|   4|   4|\n",
      "|       Datsun 710|22.8|  4|108.0| 93|3.85| 2.32|18.61|  1|  1|   4|   1|\n",
      "|   Hornet 4 Drive|21.4|  6|258.0|110|3.08|3.215|19.44|  1|  0|   3|   1|\n",
      "|Hornet Sportabout|18.7|  8|360.0|175|3.15| 3.44|17.02|  0|  0|   3|   2|\n",
      "+-----------------+----+---+-----+---+----+-----+-----+---+---+----+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----+\n",
      "|              car| mpg|\n",
      "+-----------------+----+\n",
      "|        Mazda RX4|21.0|\n",
      "|    Mazda RX4 Wag|21.0|\n",
      "|       Datsun 710|22.8|\n",
      "|   Hornet 4 Drive|21.4|\n",
      "|Hornet Sportabout|18.7|\n",
      "+-----------------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('car', 'mpg').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----+---+-----+---+----+----+-----+---+---+----+----+\n",
      "|        car| mpg|cyl| disp| hp|drat|  wt| qsec| vs| am|gear|carb|\n",
      "+-----------+----+---+-----+---+----+----+-----+---+---+----+----+\n",
      "| Duster 360|14.3|  8|360.0|245|3.21|3.57|15.84|  0|  0|   3|   4|\n",
      "|  Merc 280C|17.8|  6|167.6|123|3.92|3.44| 18.9|  1|  0|   4|   4|\n",
      "| Merc 450SE|16.4|  8|275.8|180|3.07|4.07| 17.4|  0|  0|   3|   3|\n",
      "| Merc 450SL|17.3|  8|275.8|180|3.07|3.73| 17.6|  0|  0|   3|   3|\n",
      "|Merc 450SLC|15.2|  8|275.8|180|3.07|3.78| 18.0|  0|  0|   3|   3|\n",
      "+-----------+----+---+-----+---+----+----+-----+---+---+----+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df['mpg'] < 18).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----+---+-----+---+----+----+-----+---+---+----+----+\n",
      "|        car| mpg|cyl| disp| hp|drat|  wt| qsec| vs| am|gear|carb|\n",
      "+-----------+----+---+-----+---+----+----+-----+---+---+----+----+\n",
      "| Duster 360|14.3|  8|360.0|245|3.21|3.57|15.84|  0|  0|   3|   4|\n",
      "|  Merc 280C|17.8|  6|167.6|123|3.92|3.44| 18.9|  1|  0|   4|   4|\n",
      "| Merc 450SE|16.4|  8|275.8|180|3.07|4.07| 17.4|  0|  0|   3|   3|\n",
      "| Merc 450SL|17.3|  8|275.8|180|3.07|3.73| 17.6|  0|  0|   3|   3|\n",
      "|Merc 450SLC|15.2|  8|275.8|180|3.07|3.78| 18.0|  0|  0|   3|   3|\n",
      "+-----------+----+---+-----+---+----+----+-----+---+---+----+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# the filter can be just like a SQL where clause\n",
    "\n",
    "df.filter('mpg < 18').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----+---+-----+---+----+-----+-----+---+---+----+----+-------+\n",
      "|              car| mpg|cyl| disp| hp|drat|   wt| qsec| vs| am|gear|carb|  wtTon|\n",
      "+-----------------+----+---+-----+---+----+-----+-----+---+---+----+----+-------+\n",
      "|        Mazda RX4|21.0|  6|160.0|110| 3.9| 2.62|16.46|  0|  1|   4|   4|  1.179|\n",
      "|    Mazda RX4 Wag|21.0|  6|160.0|110| 3.9|2.875|17.02|  0|  1|   4|   4|1.29375|\n",
      "|       Datsun 710|22.8|  4|108.0| 93|3.85| 2.32|18.61|  1|  1|   4|   1|  1.044|\n",
      "|   Hornet 4 Drive|21.4|  6|258.0|110|3.08|3.215|19.44|  1|  0|   3|   1|1.44675|\n",
      "|Hornet Sportabout|18.7|  8|360.0|175|3.15| 3.44|17.02|  0|  0|   3|   2|  1.548|\n",
      "|          Valiant|18.1|  6|225.0|105|2.76| 3.46|20.22|  1|  0|   3|   1|  1.557|\n",
      "+-----------------+----+---+-----+---+----+-----+-----+---+---+----+----+-------+\n",
      "only showing top 6 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# dataframes are immutable -- withColumn creates a new one\n",
    "\n",
    "df.withColumn('wtTon', df['wt'] * 0.45).show(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+\n",
      "|cyl|           avg(wt)|\n",
      "+---+------------------+\n",
      "|  6| 3.117142857142857|\n",
      "|  4| 2.285727272727273|\n",
      "|  8|3.9992142857142867|\n",
      "+---+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupby(['cyl'])\\\n",
    ".agg({\"wt\": \"AVG\"})\\\n",
    ".show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n",
      "|cyl|count(1)|\n",
      "+---+--------+\n",
      "|  8|      14|\n",
      "|  4|      11|\n",
      "|  6|       7|\n",
      "+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "car_counts = df.groupby(['cyl'])\\\n",
    ".agg({\"*\": \"count\"})\\\n",
    ".sort(\"count(1)\", ascending=False)\\\n",
    ".show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|gear|\n",
      "+----+\n",
      "|   4|\n",
      "|   4|\n",
      "|   4|\n",
      "|   3|\n",
      "|   3|\n",
      "|   3|\n",
      "+----+\n",
      "only showing top 6 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.registerTempTable(\"cars\")\n",
    "\n",
    "# SQL statements can be run by using the sql method\n",
    "highgearcars = sqlContext.sql(\"SELECT gear FROM cars WHERE cyl >= 4 AND cyl <= 9\")\n",
    "highgearcars.show(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Exercise 1:\n",
    "\n",
    "Step 1: Create a list of tuples (\"john\", 23), (\"mr. bean\", 56), (\"bill clinton\", 70)\n",
    "\n",
    "Step 2: Convert the list to a data frame with two columns\n",
    "\n",
    "Step 3: Filter the people with age < 40\n",
    "\n",
    "Step 4: Get the names ONLY of people smaller than 40\n",
    "\n",
    "Step 5: Register the dataframe as a SQL table\n",
    "\n",
    "Step 6: Select the names only of people whose age < 40\n",
    "\n",
    "Some hints: http://spark.apache.org/docs/latest/sql-programming-guide.html#generic-loadsave-functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lt = [('john',23),('mr.bean',56),('bill clinton',70)]\n",
    "rdd = sc.parallelize(lt)\n",
    "df = sqlContext.createDataFrame(rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---+\n",
      "|          _1| _2|\n",
      "+------------+---+\n",
      "|        john| 23|\n",
      "|     mr.bean| 56|\n",
      "|bill clinton| 70|\n",
      "+------------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+\n",
      "|  _1| _2|\n",
      "+----+---+\n",
      "|john| 23|\n",
      "+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df['_2']<40).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|  _1|\n",
      "+----+\n",
      "|john|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df['_2']<40).select('_1').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.registerTempTable(\"people\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|  _1|\n",
      "+----+\n",
      "|john|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SQL statements can be run by using the sql method\n",
    "somepeople = sqlContext.sql(\"SELECT _1 FROM people WHERE _2<40\")\n",
    "somepeople.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Exercise 2:\n",
    "\n",
    "Step 1: Create a list of tuple (\"john\", \"male\"), (\"hilary\", \"female\")\n",
    "\n",
    "Step 2: Join with the data from the previos exercise and output all males older than 50\n",
    "\n",
    "Hints: https://databricks.com/blog/2015/02/17/introducing-dataframes-in-spark-for-large-scale-data-science.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lt2 = [('john','male'),('hilary','female')]\n",
    "rdd2 = sc.parallelize(lt2)\n",
    "df2 = sqlContext.createDataFrame(rdd2)\n",
    "df2.registerTempTable(\"sex\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|    _1|    _2|\n",
      "+------+------+\n",
      "|  john|  male|\n",
      "|hilary|female|\n",
      "+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.crossJoin.enabled\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "sql_stat = \"SELECT p._1 FROM people p,sex s \\\n",
    "WHERE p._2<40 \\\n",
    "AND s._2 = 'male'\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "'Cartesian joins could be prohibitively expensive and are disabled by default. To explicitly enable them, please set spark.sql.crossJoin.enabled = true;'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/home/dsi-student/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/dsi-student/spark/python/lib/py4j-0.10.1-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    311\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    313\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o692.showString.\n: org.apache.spark.sql.AnalysisException: Cartesian joins could be prohibitively expensive and are disabled by default. To explicitly enable them, please set spark.sql.crossJoin.enabled = true;\n\tat org.apache.spark.sql.execution.joins.CartesianProductExec.doPrepare(CartesianProductExec.scala:97)\n\tat org.apache.spark.sql.execution.SparkPlan.prepare(SparkPlan.scala:199)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:134)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:133)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:114)\n\tat org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:240)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:323)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:39)\n\tat org.apache.spark.sql.Dataset$$anonfun$org$apache$spark$sql$Dataset$$execute$1$1.apply(Dataset.scala:2183)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57)\n\tat org.apache.spark.sql.Dataset.withNewExecutionId(Dataset.scala:2532)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$execute$1(Dataset.scala:2182)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collect(Dataset.scala:2189)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:1925)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:1924)\n\tat org.apache.spark.sql.Dataset.withTypedCallback(Dataset.scala:2562)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:1924)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2139)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:239)\n\tat sun.reflect.GeneratedMethodAccessor115.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:128)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:211)\n\tat java.lang.Thread.run(Thread.java:745)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-85-3096a733de57>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmorepeople\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msqlContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msql_stat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmorepeople\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/dsi-student/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate)\u001b[0m\n\u001b[1;32m    285\u001b[0m         \u001b[0;34m+\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m         \"\"\"\n\u001b[0;32m--> 287\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/dsi-student/spark/python/lib/py4j-0.10.1-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m         return_value = get_return_value(\n\u001b[0;32m--> 933\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m    934\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/dsi-student/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: 'Cartesian joins could be prohibitively expensive and are disabled by default. To explicitly enable them, please set spark.sql.crossJoin.enabled = true;'"
     ]
    }
   ],
   "source": [
    "morepeople = sqlContext.sql(sql_stat)\n",
    "morepeople.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "'Cartesian joins could be prohibitively expensive and are disabled by default. To explicitly enable them, please set spark.sql.crossJoin.enabled = true;'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/home/dsi-student/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/dsi-student/spark/python/lib/py4j-0.10.1-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    311\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    313\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o861.showString.\n: org.apache.spark.sql.AnalysisException: Cartesian joins could be prohibitively expensive and are disabled by default. To explicitly enable them, please set spark.sql.crossJoin.enabled = true;\n\tat org.apache.spark.sql.execution.joins.CartesianProductExec.doPrepare(CartesianProductExec.scala:97)\n\tat org.apache.spark.sql.execution.SparkPlan.prepare(SparkPlan.scala:199)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:134)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:133)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:114)\n\tat org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:240)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:323)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:39)\n\tat org.apache.spark.sql.Dataset$$anonfun$org$apache$spark$sql$Dataset$$execute$1$1.apply(Dataset.scala:2183)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57)\n\tat org.apache.spark.sql.Dataset.withNewExecutionId(Dataset.scala:2532)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$execute$1(Dataset.scala:2182)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collect(Dataset.scala:2189)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:1925)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:1924)\n\tat org.apache.spark.sql.Dataset.withTypedCallback(Dataset.scala:2562)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:1924)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2139)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:239)\n\tat sun.reflect.GeneratedMethodAccessor115.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:128)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:211)\n\tat java.lang.Thread.run(Thread.java:745)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-95-4025f80d6aea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"spark.sql.crossJoin.enabled\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/dsi-student/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate)\u001b[0m\n\u001b[1;32m    285\u001b[0m         \u001b[0;34m+\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m         \"\"\"\n\u001b[0;32m--> 287\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/dsi-student/spark/python/lib/py4j-0.10.1-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m         return_value = get_return_value(\n\u001b[0;32m--> 933\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m    934\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/dsi-student/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: 'Cartesian joins could be prohibitively expensive and are disabled by default. To explicitly enable them, please set spark.sql.crossJoin.enabled = true;'"
     ]
    }
   ],
   "source": [
    "spark.conf.set(\"spark.sql.crossJoin.enabled\", True)\n",
    "df.join(df2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
